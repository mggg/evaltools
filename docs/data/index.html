<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gerrytools.data API documentation</title>
<meta name="description" content="Facilities for processing data and districting plans in a standardized fashion." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gerrytools.data</code></h1>
</header>
<section id="section-intro">
<p>Facilities for processing data and districting plans in a standardized fashion.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Facilities for processing data and districting plans in a standardized fashion.
&#34;&#34;&#34;
from .acs import cvap, acs5
from .census import census20, census10, variables
from .estimatecvap import estimatecvap2010, estimatecvap2020, fetchgeometries
from .fetch import submissions, tabularized, Submission
from .remap import remap
from .URLs import ids, one, csvs
from .AssignmentCompressor import AssignmentCompressor

__all__ = [
    &#34;submissions&#34;,
    &#34;tabularized&#34;,
    &#34;remap&#34;,
    &#34;ids&#34;,
    &#34;one&#34;,
    &#34;csvs&#34;,
    &#34;AssignmentCompressor&#34;,
    &#34;Submission&#34;,
    &#34;cvap&#34;,
    &#34;acs5&#34;,
    &#34;census20&#34;,
    &#34;variables&#34;,
    &#34;estimatecvap2010&#34;,
    &#34;estimatecvap2020&#34;,
    &#34;fetchgeometries&#34;,
    &#34;census10&#34;
]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="gerrytools.data.URLs" href="URLs.html">gerrytools.data.URLs</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="gerrytools.data.acs" href="acs.html">gerrytools.data.acs</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="gerrytools.data.census" href="census.html">gerrytools.data.census</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="gerrytools.data.estimatecvap" href="estimatecvap.html">gerrytools.data.estimatecvap</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="gerrytools.data.fetch" href="fetch.html">gerrytools.data.fetch</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gerrytools.data.acs5"><code class="name flex">
<span>def <span class="ident">acs5</span></span>(<span>state, geometry='tract', year=2020, columns=[], white='NHWHITEVAP') ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves ACS 5-year population estimates for the provided state, geometry
level, and year. Also retrieves ACS-reported CVAP data, which closely matches
that reported by the CVAP special tabulation; CVAP data are only returned at
the tract level, and are otherwise reported as 0.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>us.State</code></dt>
<dd><code>State</code> object for the desired state.</dd>
<dt><strong><code>geometry</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Geometry level at which data is retrieved.
Acceptable values are <code>"tract"</code> and <code>"block group"</code>. Defaults to
<code>"tract"</code>, so data is retrieved at the 2020 Census tract level.</dd>
<dt><strong><code>year</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Year for which data is retrieved. Defaults to 2020.</dd>
<dt><strong><code>columns</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Columns to retrieve. If <code>None</code>, a default set
of columns including total populations by race and ethnicity and voting-age
populations by race and ethnicity are returned, along with a GEOID
column.</dd>
<dt><strong><code>white</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column removed from totals when calculating
POC populations.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A DataFrame containing the formatted data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def acs5(state, geometry=&#34;tract&#34;, year=2020, columns=[], white=&#34;NHWHITEVAP&#34;) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Retrieves ACS 5-year population estimates for the provided state, geometry
    level, and year. Also retrieves ACS-reported CVAP data, which closely matches
    that reported by the CVAP special tabulation; CVAP data are only returned at
    the tract level, and are otherwise reported as 0.

    Args:
        state (us.State): `State` object for the desired state.
        geometry (str, optional): Geometry level at which data is retrieved.
            Acceptable values are `&#34;tract&#34;` and `&#34;block group&#34;`. Defaults to
            `&#34;tract&#34;`, so data is retrieved at the 2020 Census tract level.
        year (int, optional): Year for which data is retrieved. Defaults to 2020.
        columns (list, optional): Columns to retrieve. If `None`, a default set
            of columns including total populations by race and ethnicity and voting-age
            populations by race and ethnicity are returned, along with a GEOID
            column.
        white (str, optional): The column removed from totals when calculating
            POC populations.

    Returns:
        A DataFrame containing the formatted data.
    &#34;&#34;&#34;
    # Columns for total populations.
    yearsuffix = str(year)[-2:]
    popcolumns = {
        &#34;B01001_001E&#34;: &#34;TOTPOP&#34; + yearsuffix,
        &#34;B03002_003E&#34;: &#34;WHITE&#34; + yearsuffix,
        &#34;B03002_004E&#34;: &#34;BLACK&#34; + yearsuffix,
        &#34;B03002_005E&#34;: &#34;AMIN&#34; + yearsuffix,
        &#34;B03002_006E&#34;: &#34;ASIAN&#34; + yearsuffix,
        &#34;B03002_007E&#34;: &#34;NHPI&#34; + yearsuffix,
        &#34;B03002_008E&#34;: &#34;OTH&#34; + yearsuffix,
        &#34;B03002_009E&#34;: &#34;2MORE&#34; + yearsuffix,
        &#34;B03002_002E&#34;: &#34;NHISP&#34; + yearsuffix,
    }

    # Create a dictionary of column groups.
    groups = {}

    # Get VAP columns. The columns listed here are by race, irrespective of ethnicity;
    # for example, WVAP19 is the group of people who identified White as their *only*
    # race, including people who identified as Hispanic and White.
    vapnames = [
        &#34;WHITEVAP&#34;, &#34;BLACKVAP&#34;, &#34;AMINVAP&#34;, &#34;ASIANVAP&#34;, &#34;NHPIVAP&#34;, &#34;OTHVAP&#34;, &#34;2MOREVAP&#34;,
        &#34;NHWHITEVAP&#34;, &#34;HVAP&#34;
    ]
    vaptables = list(zip(
        [column + yearsuffix for column in vapnames],
        [&#34;A&#34;, &#34;B&#34;, &#34;C&#34;, &#34;D&#34;, &#34;E&#34;, &#34;F&#34;, &#34;G&#34;, &#34;H&#34;, &#34;I&#34;]
    ))
    groups.update({
        column: _variables(f&#34;B01001{table}&#34;, 7, 16) + _variables(f&#34;B01001{table}&#34;, 22, 31)
        for column, table in vaptables
    })

    # Get CVAP columns; the same goes for these columns as does the above, except
    # these columns are 18 years and older *and* citizens.
    cvapnames = [
        &#34;WHITECVAP&#34;, &#34;BLACKCVAP&#34;, &#34;AMINCVAP&#34;, &#34;ASIANCVAP&#34;, &#34;NHPICVAP&#34;, &#34;OTHCVAP&#34;,
        &#34;2MORECVAP&#34;, &#34;NHWHITECVAP&#34;, &#34;HCVAP&#34;
    ]
    cvaptables = list(zip(
        [name + yearsuffix for name in cvapnames],
        [&#34;A&#34;, &#34;B&#34;, &#34;C&#34;, &#34;D&#34;, &#34;E&#34;, &#34;F&#34;, &#34;G&#34;, &#34;H&#34;, &#34;I&#34;]
    ))
    groups.update({
        column:
            _variables(f&#34;B05003{table}&#34;, 9, 9) + _variables(f&#34;B05003{table}&#34;, 11, 11) +  # men
            _variables(f&#34;B05003{table}&#34;, 20, 20) + _variables(f&#34;B05003{table}&#34;, 22, 22)  # women
        for column, table in cvaptables
    })

    # Get all voting-age people and citizen voting-age people.
    groups[&#34;VAP&#34; + yearsuffix] = _variables(&#34;B01001&#34;, 7, 25) + _variables(&#34;B01001&#34;, 31, 49)
    groups[&#34;CVAP&#34; + yearsuffix] = _variables(&#34;B05003&#34;, 9, 9) + \
        _variables(&#34;B05003&#34;, 11, 11) + \
        _variables(&#34;B05003&#34;, 20, 20) + \
        _variables(&#34;B05003&#34;, 22, 22)

    # TODO: all variables used across the data submodule should be packaged up
    # as a class, so we can access individual dictionaries of variables to add.
    # For example, we should have a `Variables.acs5.vap` property which gives us
    # the voting-age population variables for the ACS 5-year estimates.

    # Get the list of all columns.
    allcols = list(popcolumns.keys()) + [c for k in groups.values() for c in k] + columns

    # Retrieve the data from the Census API.
    data = censusdata.download(
        &#34;acs5&#34;, year,
        censusdata.censusgeo(
            [(&#34;state&#34;, str(state.fips).zfill(2)), (&#34;county&#34;, &#34;*&#34;), (geometry, &#34;*&#34;)]
        ),
        [&#34;GEO_ID&#34;] + allcols
    )

    # Rework columns.
    data = data.reset_index(drop=True)
    data[&#34;GEO_ID&#34;] = data[&#34;GEO_ID&#34;].str.split(&#34;US&#34;).str[1]
    data = data.rename({&#34;GEO_ID&#34;: geometry.replace(&#34; &#34;, &#34;&#34;).upper() + (&#34;10&#34; if year &lt; 2020 else &#34;20&#34;)}, axis=1)
    data = data.rename(popcolumns, axis=1)

    # Collapse column groups.
    for column, group in groups.items():
        data[column] = data[group].sum(axis=1)
        data = data.drop(group, axis=1)

    # Create a POCVAP column.
    data[&#34;POCVAP&#34; + yearsuffix] = data[&#34;VAP&#34; + yearsuffix] - data[white + yearsuffix]
    return data</code></pre>
</details>
</dd>
<dt id="gerrytools.data.census10"><code class="name flex">
<span>def <span class="ident">census10</span></span>(<span>state, columns={}, geometry='block')</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves <code>geometry</code>-level 2010 Summary File 1 data via the Census API.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>State</code></dt>
<dd><code>us.State</code> object (e.g. <code>us.states.WI</code>).</dd>
<dt><strong><code>columns</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Dictionary which maps Census column names (from
the correct table) to human-readable names. We require this to be a
dictionary, <em>not</em> a list, as specifying human-readable names will
implicitly protect against incorrect column names and excessive API
calls.</dd>
<dt><strong><code>geometry</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Geometry level at which we retrieve data.
Defaults to <code>"block"</code> to retrieve block-level data for the state
provided. Accepted values are <code>"block"</code>, <code>"block group</code>", and <code>"tract"</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A DataFrame with columns renamed according to their Census description
designation and a unique identifier column for joining to geometries.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def census10(state, columns={}, geometry=&#34;block&#34;):
    &#34;&#34;&#34;
    Retrieves `geometry`-level 2010 Summary File 1 data via the Census API.

    Args:
        state (State): `us.State` object (e.g. `us.states.WI`).
        columns (dict, optional): Dictionary which maps Census column names (from
            the correct table) to human-readable names. We require this to be a
            dictionary, _not_ a list, as specifying human-readable names will
            implicitly protect against incorrect column names and excessive API
            calls.
        geometry (string, optional): Geometry level at which we retrieve data.
            Defaults to `&#34;block&#34;` to retrieve block-level data for the state
            provided. Accepted values are `&#34;block&#34;`, `&#34;block group`&#34;, and `&#34;tract&#34;`.

    Returns:
        A DataFrame with columns renamed according to their Census description
        designation and a unique identifier column for joining to geometries.
    &#34;&#34;&#34;
    # Create the right geometry identifiers.
    geometries = [(&#34;state&#34;, str(state.fips)), (&#34;county&#34;, &#34;*&#34;), (&#34;tract&#34;, &#34;*&#34;)]
    if geometry in {&#34;block group&#34;, &#34;block&#34;}:
        geometries += [(geometry, &#34;*&#34;)]

    # Create an identifier column.
    identifier = geometry.replace(&#34; &#34;, &#34;&#34;).upper() + &#34;10&#34;

    # Download data.
    raw = censusdata.download(
        &#34;sf1&#34;, 2010, censusdata.censusgeo(geometries),
        [&#34;GEO_ID&#34;] + list(columns.keys()),
    )

    # Rename columns and send back to the caller!
    raw = raw.rename({&#34;GEO_ID&#34;: identifier, **columns}, axis=1)
    raw[identifier] = raw[identifier].str[9:]
    clean = raw.reset_index(drop=True)

    return clean</code></pre>
</details>
</dd>
<dt id="gerrytools.data.census20"><code class="name flex">
<span>def <span class="ident">census20</span></span>(<span>state, table='P1', columns={}, geometry='block', key='75c0c07e6f0ab7b0a9a1c14c3d8af9d9f13b3d65') ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves <code>geometry</code>-level 2020 Decennial Census PL94-171 data via the Census
API.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>State</code></dt>
<dd><code>us.State</code> object (e.g. <code>us.states.WI</code>).</dd>
<dt><strong><code>table</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Table from which we retrieve data. Defaults to
the P1 table, which gets populations by race regardless of ethnicity.</dd>
<dt><strong><code>columns</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Dictionary which maps Census column names (from
the correct table) to human-readable names. We require this to be a
dictionary, <em>not</em> a list, as specifying human-readable names will
implicitly protect against incorrect column names and excessive API
calls.</dd>
<dt><strong><code>geometry</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Geometry level at which we retrieve data.
Defaults to <code>"block"</code> to retrieve block-level data for the state
provided. Accepted values are <code>"block"</code>, <code>"block group</code>", and <code>"tract"</code>.</dd>
<dt><strong><code>key</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Census API key.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A DataFrame with columns renamed according to their Census description
designation and a <code>GEOID20</code> column for joining to geometries.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def census20(
    state, table=&#34;P1&#34;, columns={}, geometry=&#34;block&#34;,
    key=&#34;75c0c07e6f0ab7b0a9a1c14c3d8af9d9f13b3d65&#34;
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Retrieves `geometry`-level 2020 Decennial Census PL94-171 data via the Census
    API.

    Args:
        state (State): `us.State` object (e.g. `us.states.WI`).
        table (string, optional): Table from which we retrieve data. Defaults to
            the P1 table, which gets populations by race regardless of ethnicity.
        columns (dict, optional): Dictionary which maps Census column names (from
            the correct table) to human-readable names. We require this to be a
            dictionary, _not_ a list, as specifying human-readable names will
            implicitly protect against incorrect column names and excessive API
            calls.
        geometry (string, optional): Geometry level at which we retrieve data.
            Defaults to `&#34;block&#34;` to retrieve block-level data for the state
            provided. Accepted values are `&#34;block&#34;`, `&#34;block group`&#34;, and `&#34;tract&#34;`.
        key (string, optional): Census API key.

    Returns:
        A DataFrame with columns renamed according to their Census description
        designation and a `GEOID20` column for joining to geometries.
    &#34;&#34;&#34;
    # Check whether the geometry is right. If not, warn the user and set it
    # properly.
    if geometry not in {&#34;block&#34;, &#34;tract&#34;, &#34;block group&#34;}:
        print(f&#34;Geometry \&#34;{geometry}\&#34; not accepted; defaulting to \&#34;block\&#34;.&#34;)
        geometry = &#34;block&#34;

    # Check whether we&#39;re providing an appropriate table name.
    if table not in {&#34;P1&#34;, &#34;P2&#34;, &#34;P3&#34;, &#34;P4&#34;}:
        print(f&#34;Table \&#34;{table}\&#34; not accepted; defaulting to \&#34;P1.\&#34;&#34;)
        table = &#34;P1&#34;

    # Set the base Census API URL and get the keys for the provided table.
    base = &#34;https://api.census.gov/data/2020/dec/pl&#34;
    varmap = columns if columns else variables(table)
    vars = list(varmap.keys())

    # Create the end part of the query string.
    q = [
        (&#34;key&#34;, key),
        (&#34;for&#34;, f&#34;{geometry.replace(&#39; &#39;, r&#39;%20&#39;)}:*&#34;),
        (&#34;in&#34;, f&#34;state:{str(state.fips).zfill(2)}&#34;),
        (&#34;in&#34;, &#34;county:*&#34;),
    ]

    # Based on the geometry type, add an additional entry; this is required to
    # match the Census geographic hierarchy.
    if geometry in {&#34;block&#34;, &#34;block group&#34;}:
        q.append((&#34;in&#34;, &#34;tract:*&#34;))

    # Now, since the Census doesn&#39;t allow us to request more than 50 variables
    # at once, we request things in two parts and then merge them together.
    mergeable = []

    # Split up start and stop positions based on the number of variables.
    if len(vars) &lt; 45:
        positions = [(0, len(vars))]
    else:
        positions = [(0, 45), (45, len(vars))]

    for start, stop in positions:
        # Get the chunk of variables and create a tail of columns (geographic
        # identifiers).
        varchunk = vars[start:stop]
        last = [geometry] if geometry in {&#34;block group&#34;, &#34;block&#34;} else []
        tail = [&#34;state&#34;, &#34;county&#34;, &#34;tract&#34;] + last

        # Create an unescaped query string.
        unescaped = q.copy()
        unescaped.append((&#34;get&#34;, &#34;,&#34;.join(varchunk)))

        # Create an escaped query string from the previous.
        escaped = &#34;?&#34; + &#34;&amp;&#34;.join(
            f&#34;{param}={value}&#34; for param, value in unescaped
        )

        # Send the request and create a dataframe.
        req = requests.get(base + escaped).json()
        header, data = req[0], req[1:]
        chunk = pd.DataFrame(data, columns=header)

        # Get a GEOID column and drop old columns.
        chunk[&#34;GEOID20&#34;] = _rjoin(chunk[tail], tail)
        chunk = chunk.drop(tail, axis=1)
        mergeable.append(chunk)

    # Merge the dataframes, rename everything, make the columns ints, and return.
    merged = reduce(lambda l, r: pd.merge(l, r, on=&#34;GEOID20&#34;), mergeable)
    merged = merged.rename(varmap, axis=1)
    merged = merged.astype({var: int for var in varmap.values()})

    # Make the GEOID20 column the first column.
    merged = merged[[&#34;GEOID20&#34;] + list(varmap.values())]

    return merged</code></pre>
</details>
</dd>
<dt id="gerrytools.data.csvs"><code class="name flex">
<span>def <span class="ident">csvs</span></span>(<span>state, ptype='plan')</span>
</code></dt>
<dd>
<div class="desc"><p>URL for accessing districtr plan metadata.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd><code>us.States</code> object (e.g. <code>us.states.WI</code>)</dd>
<dt><strong><code>ptype</code></strong></dt>
<dd>Type of plan we're retrieving; defaults to <code>"plan"</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>String with the appropriate URL.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def csvs(state, ptype=&#34;plan&#34;):
    &#34;&#34;&#34;
    URL for accessing districtr plan metadata.

    Args:
        state: `us.States` object (e.g. `us.states.WI`)
        ptype: Type of plan we&#39;re retrieving; defaults to `&#34;plan&#34;`.

    Returns:
        String with the appropriate URL.
    &#34;&#34;&#34;
    pipeline = &#34;beta&#34; if state == us.states.MI else &#34;prod&#34;
    if state == us.states.MI:
        prefix = &#34;https://o1siz7rw0c.execute-api.us-east-2.amazonaws.com&#34;
    else:
        prefix = &#34;https://k61e3cz2ni.execute-api.us-east-2.amazonaws.com&#34;

    suffix = f&#34;?type={ptype}&amp;length=10000&#34;

    return f&#34;{prefix}/{pipeline}/submissions/csv/{state.name.lower()}{suffix}&#34;</code></pre>
</details>
</dd>
<dt id="gerrytools.data.cvap"><code class="name flex">
<span>def <span class="ident">cvap</span></span>(<span>state, geometry='tract', year=2020) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves and CSV-formats 5-year CVAP data for the provided state at
the specified geometry level. Geometries from the <strong>2010 Census</strong>. Variables
and descriptions are <a href="https://tinyurl.com/3mnrm56s">listed here</a>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>us.State</code></dt>
<dd>The <code>State</code> object for which we're retrieving 2019 ACS
CVAP Special Tab.</dd>
<dt><strong><code>geometry</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Level of geometry for which we're getting data.
Accepted values are <code>"block group"</code> for 2010 Census Block Groups, and
<code>"tract"</code> for 2010 Census Tracts. Defaults to <code>"tract"</code>.</dd>
<dt><strong><code>year</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Year for which data is retrieved. Defaults to 2020.</dd>
</dl>
<p>Returns
A <code>DataFrame</code> with a <code>GEOID</code> column and corresponding CVAP columns from
the ACS CVAP Special Tab for the specified year.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cvap(state, geometry=&#34;tract&#34;, year=2020) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Retrieves and CSV-formats 5-year CVAP data for the provided state at
    the specified geometry level. Geometries from the **2010 Census**. Variables
    and descriptions are [listed here](https://tinyurl.com/3mnrm56s).

    Args:
        state (us.State): The `State` object for which we&#39;re retrieving 2019 ACS
            CVAP Special Tab.
        geometry (str, optional): Level of geometry for which we&#39;re getting data.
            Accepted values are `&#34;block group&#34;` for 2010 Census Block Groups, and
            `&#34;tract&#34;` for 2010 Census Tracts. Defaults to `&#34;tract&#34;`.
        year (int, optional): Year for which data is retrieved. Defaults to 2020.

    Returns
        A `DataFrame` with a `GEOID` column and corresponding CVAP columns from
        the ACS CVAP Special Tab for the specified year.
    &#34;&#34;&#34;
    # Maps line numbers to descriptors.
    descriptions = {
        1: &#34;CVAP&#34;,
        2: &#34;NHCVAP&#34;,
        3: &#34;NHAMINCVAP&#34;,
        4: &#34;NHASIANCVAP&#34;,
        5: &#34;NHBLACKCVAP&#34;,
        6: &#34;NHNHPICVAP&#34;,
        7: &#34;NHWHITECVAP&#34;,
        8: &#34;NHWHITEAMINCVAP&#34;,
        9: &#34;NHWHITEASIANCVAP&#34;,
        10: &#34;NHWHITEBLACKCVAP&#34;,
        11: &#34;NHBLACKAMINCVAP&#34;,
        12: &#34;NHOTHCVAP&#34;,
        13: &#34;HCVAP&#34;
    }

    # First, load the raw data requested; allowed geometry values are &#34;block group&#34;
    # and &#34;tract.&#34;
    if geometry not in {&#34;block group&#34;, &#34;tract&#34;}:
        print(f&#34;Requested geometry \&#34;{geometry}\&#34; is not allowed; loading tracts.&#34;)
        geometry = &#34;tract&#34;

    abbrv = geometry if geometry == &#34;tract&#34; else &#34;block group&#34;

    # Load the raw data.
    raw = _raw(abbrv, year)

    # Create a STATE column for filtering and remove all rows which don&#39;t match
    # the state FIPS code.
    raw[&#34;GEOID&#34;] = raw[&#34;geoid&#34;].str.split(&#34;US&#34;).str[1]
    raw[&#34;STATE&#34;] = raw[&#34;GEOID&#34;].str[:2]
    instate = raw[raw[&#34;STATE&#34;] == str(state.fips)]

    # Now that we have the in-state data, we aim to pivot the table. Because the
    # ACS data is in a line-numbered format (i.e. each chunk of 13 lines matches
    # to an individual geometry, and each of the 13 lines describes an individual
    # statistic) we need to first collapse each chunk of 13 lines, then build a
    # dataframe from the resulting collapsed lines. First we send the dataframe
    # to a list of records.
    instate_records = instate.to_dict(orient=&#34;records&#34;)
    collapsed = []

    # Get year stuff.
    decade = &#34;10&#34; if year &lt; 2020 else &#34;20&#34;
    yearsuffix = str(year)[2:]

    # Next, we collapse these records to a single record.
    for i in range(0, len(instate_records), 13):
        # Create an empty records.
        record = {}

        # For each of the records in the block, &#34;collapse&#34; them into a single
        # record.
        block = instate_records[i:i + 13]
        for line in block:
            record[geometry.replace(&#34; &#34;, &#34;&#34;).upper() + decade] = line[&#34;GEOID&#34;]
            record[descriptions[line[&#34;lnnumber&#34;]] + yearsuffix] = line[&#34;cvap_est&#34;]
            record[descriptions[line[&#34;lnnumber&#34;]] + f&#34;{yearsuffix}e&#34;] = line[&#34;cvap_moe&#34;]

        collapsed.append(record)

    # Create a dataframe and a POCCVAP column; all people minus non-Hispanic
    # White.
    data = pd.DataFrame().from_records(collapsed)
    data[f&#34;POCCVAP{yearsuffix}&#34;] = data[f&#34;CVAP{yearsuffix}&#34;] - data[f&#34;NHWHITECVAP{yearsuffix}&#34;]

    return data</code></pre>
</details>
</dd>
<dt id="gerrytools.data.estimatecvap2010"><code class="name flex">
<span>def <span class="ident">estimatecvap2010</span></span>(<span>base, state, groups, ceiling, zfill, geometry10='tract', year=2019) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Function for turning old (2019) CVAP data on 2010 geometries into estimates
for current CVAP data on 2020 geometries. <strong>This method serves a different
purpose than <code><a title="gerrytools.data.estimatecvap.estimatecvap2020" href="estimatecvap.html#gerrytools.data.estimatecvap.estimatecvap2020">estimatecvap2020()</a></code>:</strong> this method
is intended to put 2010-era CVAP data on 2020-era geometries, and uses
geometric properties to do so.</p>
<p>Users must supply a base <code>GeoDataFrame</code>
representing their chosen U.S. state. Additionally, users must specify the
demographic groups whose CVAP statistics are to be estimated. For each group,
users specify a triple <span><span class="MathJax_Preview">(X, Y, Z)</span><script type="math/tex">(X, Y, Z)</script></span> where <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> is the old CVAP column for
that group, <span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> is the old VAP column for that group, and <span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> is the new VAP
column for that group, which must be an existing column on <code>base</code>.
Then,
the estimated new CVAP for that group will be constructed by multiplying
<span><span class="MathJax_Preview">(X / Y) \cdot Z</span><script type="math/tex">(X / Y) \cdot Z</script></span> for each new geometry.</p>
<div style="text-align: center;">
</br>
<img width="75%" src="../images/cvap-estimation.png"/>
</div>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base</code></strong> :&ensp;<code>GeoDataFrame</code></dt>
<dd>A <code>GeoDataFrame</code> with the appropriate columns for
estimating CVAP.</dd>
<dt><strong><code>state</code></strong> :&ensp;<code>State</code></dt>
<dd>The <code>us.State</code> object for which CVAP data is retrieved.</dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>list</code></dt>
<dd><code>(X, Y, Z)</code> triples for each desired CVAP group to be
estimated, where each of the parameters are column names: <code>X</code> is
the column on the 2010 geometries which contains the relevant CVAP
data; <code>Y</code> is the column on the 2010 geometries which contains the
relevant VAP data; <code>Z</code> is the column on the 2020 geometries to be
weighted by the ratio of the per-unit ratios in <code>X</code> and <code>Y</code>. For
example, if we wish to estimate Black CVAP, this triple would be
<code>(NHBCVAP19, BVAP19, BVAP20)</code>, which takes the ratios of the <code>NHBCVAP19</code>
and <code>BVAP19</code> columns on the 2010 geometries, and multplies the 2020
geometries' respective <code>BVAP20</code> values by these ratios.</dd>
<dt><strong><code>ceiling</code></strong> :&ensp;<code>float</code></dt>
<dd>Number representing where to cap the weighting ratio of
CVAP to VAP20. After this percentage ceiling is passed, the percentage
will be set to 1. We recommend setting this to 1.</dd>
<dt><strong><code>zfill</code></strong> :&ensp;<code>float</code></dt>
<dd>Fill in ratio for CVAP to VAP20 when there is 0 CVAP in the
area. We recommend setting this parameter to <code>0.1</code>.</dd>
<dt><strong><code>geometry10</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The 2010 geometry on which cvap will be pulled.
Acceptable values are <code>"tract"</code> or <code>"block group"</code>. As tracts are
less susceptible to change across Census vintages, setting this parameter
to <code>"tract"</code> is recommended, as it is more likely that the 2020 Census
blocks fit neatly into the 2010 Census tracts.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>base</code> geometries with 2019 CVAP-weighted 2020 CVAP estimates attached.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimatecvap2010(
    base, state, groups, ceiling, zfill, geometry10=&#34;tract&#34;, year=2019
) -&gt; DataFrame:
    r&#34;&#34;&#34;
    Function for turning old (2019) CVAP data on 2010 geometries into estimates
    for current CVAP data on 2020 geometries. **This method serves a different
    purpose than `gerrytools.data.estimatecvap.estimatecvap2020()`:** this method
    is intended to put 2010-era CVAP data on 2020-era geometries, and uses
    geometric properties to do so.

    Users must supply a base `GeoDataFrame`
    representing their chosen U.S. state. Additionally, users must specify the
    demographic groups whose CVAP statistics are to be estimated. For each group,
    users specify a triple \((X, Y, Z)\) where \(X\) is the old CVAP column for
    that group, \(Y\) is the old VAP column for that group, and \(Z\) is the new VAP
    column for that group, which must be an existing column on `base`.  Then,
    the estimated new CVAP for that group will be constructed by multiplying
    \((X / Y) \cdot Z\) for each new geometry.

    &lt;div style=&#34;text-align: center;&#34;&gt;
        &lt;/br&gt;
        &lt;img width=&#34;75%&#34; src=&#34;../images/cvap-estimation.png&#34;/&gt;
    &lt;/div&gt;

    Args:
        base (GeoDataFrame): A `GeoDataFrame` with the appropriate columns for
            estimating CVAP.
        state (State): The `us.State` object for which CVAP data is retrieved.
        groups (list): `(X, Y, Z)` triples for each desired CVAP group to be
            estimated, where each of the parameters are column names: `X` is
            the column on the 2010 geometries which contains the relevant CVAP
            data; `Y` is the column on the 2010 geometries which contains the
            relevant VAP data; `Z` is the column on the 2020 geometries to be
            weighted by the ratio of the per-unit ratios in `X` and `Y`. For
            example, if we wish to estimate Black CVAP, this triple would be
            `(NHBCVAP19, BVAP19, BVAP20)`, which takes the ratios of the `NHBCVAP19`
            and `BVAP19` columns on the 2010 geometries, and multplies the 2020
            geometries&#39; respective `BVAP20` values by these ratios.
        ceiling (float): Number representing where to cap the weighting ratio of
            CVAP to VAP20. After this percentage ceiling is passed, the percentage
            will be set to 1. We recommend setting this to 1.
        zfill (float): Fill in ratio for CVAP to VAP20 when there is 0 CVAP in the
            area. We recommend setting this parameter to `0.1`.
        geometry10 (str, optional): The 2010 geometry on which cvap will be pulled.
            Acceptable values are `&#34;tract&#34;` or `&#34;block group&#34;`. As tracts are
            less susceptible to change across Census vintages, setting this parameter
            to `&#34;tract&#34;` is recommended, as it is more likely that the 2020 Census
            blocks fit neatly into the 2010 Census tracts.

    Returns:
       `base` geometries with 2019 CVAP-weighted 2020 CVAP estimates attached.
    &#34;&#34;&#34;
    if geometry10 not in {&#34;block group&#34;, &#34;tract&#34;}:
        print(f&#34;Requested geometry \&#34;{geometry10}\&#34; is not allowed; loading tracts.&#34;)
        geometry10 = &#34;tract&#34;

    # Grab ACS and CVAP special-tab data, and make sure our triples are correct
    cvap_geoid = &#34;TRACT10&#34; if geometry10 == &#34;tract&#34; else &#34;BLOCKGROUP10&#34;
    acs_source = acs5(state, geometry10, year=year)
    cvap_source = cvap(state, geometry10, year=year)

    # Validate the columns passed, issuing user warnings when it&#39;s inadvisable
    # to estimate CVAP given the passed columns.
    for (cvap10, vap10, vap20) in groups:
        # If any of the CVAP columns passed correspond to columns which tabulate
        # people of multiple races, notify the user that there isn&#39;t an appropriate
        # 2019 VAP column to match them against.
        if any(substring in cvap10 for substring in {&#34;AIW&#34;, &#34;AW&#34;, &#34;BW&#34;, &#34;AIB&#34;}):
            print(
                f&#34;Warning: Estimating CVAP among {cvap10} is not advisable, since &#34;
                &#34;there isn&#39;t a reasonable VAP column from which to construct _CVAP &#34;
                &#34;/ _VAP rates (because you seem to be combining two racial groups).&#34;
            )

        # If the CVAP or ACS5 columns passed aren&#39;t present in the set of possible
        # columns, raise an error.
        if not (cvap10 in acs_source or cvap10 in cvap_source):
            possible_columns = set(acs_source).union(set(cvap_source))
            raise ValueError(
                f&#34;Your CVAP column &#39;{cvap10}&#39; must be contained in either the ACS &#34;
                f&#34;or Special Tab columns: {possible_columns}&#34;
            )

        if vap10 not in acs_source:
            raise ValueError(
                f&#34;Your old VAP column &#39;{vap10}&#39; must be contained in the ACS &#34;
                &#34;columns: {set(acs_source)}&#34;
            )

        # If the VAP20 column passed doesn&#39;t exist on the user-provided geometries,
        # raise an error.
        if vap20 not in base:
            raise ValueError(
                f&#34;Your new VAP column &#39;{vap20}&#39; must be contained in your base &#34; +
                f&#34;dataframe: {set(base)}&#34;
            )

    # Remove ACS 5 columns that overlap with special-tab ones.
    non_overlaps = list(set(acs_source).difference(set(cvap_source)))
    acs_source = acs_source[[cvap_geoid] + non_overlaps]
    source = cvap_source.merge(acs_source, on=cvap_geoid)

    # Get the right columns from the base geometry, and map the base geometries
    # to the units with CVAP data. Apparently dropping bad columns by using slicing
    # incudes a SettingWithCopy warning, so we&#39;re just dropping using .drop()
    # instead.
    correct = [&#34;geometry&#34;] + [col for col in list(base) if any(sub in col for sub in [&#34;POP&#34;, &#34;VAP&#34;, &#34;GEOID&#34;])]
    bads = list(set(base) - set(correct))

    # Warn the user of column removal:
    print(&#34;Removing the following columns: &#34; + &#34;, &#34;.join(bads))

    pared = base.drop(bads, axis=1)
    pared = mapbase(pared, state, geometry10)

    # Compute weights.
    for (cvap10, vap10, _) in groups:
        source[cvap10 + &#34;%&#34;] = source[cvap10] / source[vap10]

    # Fill in values according to the following rules:
    #
    #   1.  if there are 0 *CVAP reported and 0 *VAP reported, we set the weight to
    #       the average *CVAP/*VAP ratio within the county;
    #   2.  if there are 0 *CVAP reported and *VAP &gt; 0, we set the weight to zero_fill;
    #   3.  if *CVAP &gt; 0 but *VAP = 0 or *CVAP/*VAP &gt; percentage_cap, we set the weight to 1.
    statewide = {
        cvap + &#34;%&#34;: source[cvap].sum() / source[vap].sum() if source[vap].sum() != 0 else 0
        for (cvap, vap, _) in groups
    }

    # Rename colunms with percentages.
    cvappcts = [cvap + &#34;%&#34; for (cvap, _, _) in groups]

    # Get the county names and compute population-weighted CVAP averages. This
    # serves as a replacement for the statewide average.
    source[&#34;_county&#34;] = source[cvap_geoid].str[2:5]
    counties = list(set(source[&#34;_county&#34;]))
    countyaverages = {pct: {} for pct in cvappcts}

    for county in counties:
        chunk = source[source[&#34;_county&#34;] == county]

        for cvap19, vap19, _ in groups:
            # Calculate the CVAP19-to-VAP19 ratio. Set numpy to ignore runtimewarnings,
            # but warn the user if one is encountered. We do this so that the user
            # doesn&#39;t get spooked by a numpy warning, but we&#39;re still noisy about
            # the weird value encountered.
            cvap19total = chunk[cvap19].sum()
            vap19total = chunk[vap19].sum()

            np.seterr(divide=&#34;ignore&#34;, invalid=&#34;ignore&#34;)
            ratio = cvap19total / vap19total

            # Check whether the ratio of the above is less than the ceiling.
            if not np.isfinite(ratio):
                print(county,
                      f&#34;Encountered an invalid ratio: there are {cvap19total} {cvap19} &#34;
                      f&#34;persons and {vap19total} {vap19} persons, for a ratio of &#34;
                      f&#34;{cvap19total}/{vap19total}. &#34;
                      f&#34;we have substituted it for the statewide {cvap19}-to-{vap19} &#34;
                      f&#34;share of {statewide[cvap19 + &#39;%&#39;]}.&#34;
                      )
                ratio = statewide[cvap19 + &#34;%&#34;]

            # Set the county-average ratio.
            countyaverages[cvap19 + &#34;%&#34;][county] = ratio

    # Reset the numpy error catching thing.
    np.seterr(all=&#34;warn&#34;)

    # For each of the percentage columns, we want to apply the rules specified
    # by the user.
    for pct in cvappcts:
        # Fill NaNs with the *county-wide* average.
        countywidepcts = countyaverages[pct]

        source[pct] = source[pct].replace(np.inf, np.nan)
        nanindices = source[source[pct].isna()].index
        source.loc[nanindices, pct] = source.loc[nanindices, &#34;_county&#34;].map(countywidepcts)

        # Fill zeroes with the `zfill` value, and cap all the percentages.
        source[pct] = source[pct] \
            .replace(0, zfill) \
            .apply(lambda c: 1 if c &gt; ceiling else c)

    # Assert we don&#39;t have any percentages over percentage_cap.
    assert all(
        np.all(source[p + &#34;%&#34;] &lt;= ceiling)
        for (p, _, __) in groups
    )

    # Assert we don&#39;t have any zeros.
    assert all(
        np.all(source[p + &#34;%&#34;] &gt; 0)
        for (p, _, __) in groups
    )

    # Set indices and create a mapping from IDs to weights.
    source = source.set_index(cvap_geoid)
    source = source[cvappcts]
    weights = source.to_dict(orient=&#34;index&#34;)

    # Group by the CVAP GEOID.
    groupedtogeometry = list(pared.groupby(cvap_geoid))

    # Get the year suffix so we can replace columns.
    yearsuffix = str(year)[2:]

    # For each of the geometry groups (e.g. a set of rows of blocks corresponding
    # to a single tract), and for each of the CVAP groups, apply the appropriate
    # weight to the blocks&#39; 2020 VAP populations.
    for ix, group in groupedtogeometry:
        for (cvap10, vap10, vap20) in groups:
            weight = cvap10 + &#34;%&#34;
            cvap20 = cvap10.replace(yearsuffix, &#34;20_EST&#34;)
            group[weight] = weights[ix][weight]
            group[cvap20] = group[weight] * group[vap20]

    # Re-create a dataframe and strip out % columns, leaving only the estimate
    # columns.
    weightedbase = pd.concat(frame for _, frame in groupedtogeometry)
    weightedbase = weightedbase.drop(columns=[p + &#34;%&#34; for (p, _, _) in groups])
    return weightedbase</code></pre>
</details>
</dd>
<dt id="gerrytools.data.estimatecvap2020"><code class="name flex">
<span>def <span class="ident">estimatecvap2020</span></span>(<span>state) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Estimates 2020 CVAP on 2020 blocks using 2020 PL94 data. <strong>This method serves
a different purpose than <code><a title="gerrytools.data.estimatecvap.estimatecvap2010" href="estimatecvap.html#gerrytools.data.estimatecvap.estimatecvap2010">estimatecvap2010()</a></code>:</strong>
rather than using geometric procedures to put CVAP data on old geometries,
this method takes advantage of the Census's geographic hierarchy, and
associates finer-grained 2020 CVAP data with 2020 blocks. <em>No geometric
data or procedures are used here</em>. The resulting data can then be adjoined
to 2020 block geometries (or assigned to VTDs, assigned to districts, etc.)
and be used to build other units of varying size.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>State</code></dt>
<dd>The <code>us.State</code> for which CVAP will be estimated.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A <code>DataFrame</code> of combined Census and ACS data at the Census block level.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimatecvap2020(state) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Estimates 2020 CVAP on 2020 blocks using 2020 PL94 data. **This method serves
    a different purpose than `gerrytools.data.estimatecvap.estimatecvap2010()`:**
    rather than using geometric procedures to put CVAP data on old geometries,
    this method takes advantage of the Census&#39;s geographic hierarchy, and
    associates finer-grained 2020 CVAP data with 2020 blocks. _No geometric
    data or procedures are used here_. The resulting data can then be adjoined
    to 2020 block geometries (or assigned to VTDs, assigned to districts, etc.)
    and be used to build other units of varying size.

    Args:
        state (State): The `us.State` for which CVAP will be estimated.

    Returns:
        A `DataFrame` of combined Census and ACS data at the Census block level.
    &#34;&#34;&#34;

    # First, get the Census data for blocks and block groups.
    bg = census20(state, table=&#34;P4&#34;, geometry=&#34;block group&#34;)
    block = census20(state, table=&#34;P4&#34;, geometry=&#34;block&#34;)

    # Now, get 2020 Census data at the block group level and merging it with the
    # block group-level Census data.
    cvap20 = cvap(state, geometry=&#34;block group&#34;, year=2020)
    bg = bg.merge(cvap20, left_on=&#34;GEOID20&#34;, right_on=&#34;BLOCKGROUP20&#34;)

    # Name the VAP columns.
    vapcolumns = [
        &#34;NHWHITEVAP20&#34;, &#34;NHASIANVAP20&#34;, &#34;NHBLACKVAP20&#34;, &#34;NHNHPIVAP20&#34;, &#34;NHAMINVAP20&#34;,
        &#34;NHWHITEASIANVAP20&#34;, &#34;NHWHITEAMINVAP20&#34;, &#34;NHWHITEBLACKVAP20&#34;, &#34;NHBLACKAMINVAP20&#34;
    ]

    # Create &#34;remainder&#34; column.
    for universe in [block, bg]:
        universe[&#34;NHVAP20&#34;] = universe[&#34;VAP20&#34;] - universe[&#34;HVAP20&#34;]
        universe[&#34;OTHVAP20&#34;] = universe[&#34;NHVAP20&#34;] - universe[vapcolumns].sum(axis=1)

    # Get the block group ID for blocks.
    block[&#34;BLOCKGROUP20&#34;] = block[&#34;GEOID20&#34;].astype(str).str[:-3]

    # Mapping from block group IDs to block group total populations.
    bgtotalvapmap = dict(zip(bg[&#34;BLOCKGROUP20&#34;].astype(str), bg[&#34;VAP20&#34;]))

    # Add all columns.
    allvapcols = vapcolumns + [&#34;VAP20&#34;, &#34;NHVAP20&#34;, &#34;HVAP20&#34;, &#34;NHOTHVAP20&#34;]

    # Estimate CVAP data for all VAP columns.
    for vapcolumn in allvapcols:
        # Crete a mapping from block group names to totals for the VAP column.
        popmap = dict(zip(bg[&#34;GEOID20&#34;].astype(str), bg[vapcolumn]))

        # Create column names.
        colpct = vapcolumn + &#34;%&#34;
        cvapcolumn = vapcolumn.replace(&#34;VAP&#34;, &#34;CVAP&#34;)

        # Calculate ratios.
        block[colpct] = block[&#34;BLOCKGROUP20&#34;].map(popmap)
        block[colpct] = block[vapcolumn] / block[colpct]

        # Create a mapping from block group IDs to CVAP groups.
        cvapcolumn = vapcolumn.replace(&#34;VAP&#34;, &#34;CVAP&#34;)
        cvapmap = dict(zip(bg[&#34;BLOCKGROUP20&#34;].astype(str), bg[cvapcolumn]))

        # Create two temporary columns: the first sets the block&#39;s CVAP to the
        # total CVAP for its block group; the second sets the block&#39;s VAP to the
        # total VAP for its block group. (Note: each of these C/VAP columns are
        # with respect to the current VAP column.)
        block[&#34;tmp&#34;] = block[&#34;BLOCKGROUP20&#34;].map(cvapmap)
        block[&#34;BGVAP20&#34;] = block[&#34;BLOCKGROUP20&#34;].map(bgtotalvapmap)

        # Next, compute the estimated CVAP by multiplying the VAP column percent
        # for the block group by the total CVAP population of the block group.
        block[cvapcolumn] = block[colpct] * block[&#34;tmp&#34;]

        # If the above doesn&#39;t work — which is the case if the VAP column percent
        # is NaN (0/0) or inf (k/0), we estimate the CVAP of the block using the
        # VAP ratio outright rather than the column-specific VAP ratio.
        ni = block[block[colpct].isna()].index
        block.loc[ni, cvapcolumn] = (block.loc[ni, &#34;VAP20&#34;] / block.loc[ni, &#34;BGVAP20&#34;]) * block.loc[ni, &#34;tmp&#34;]

        # Assert that our summed disaggregated numbers and totals are close!
        assert np.isclose(bg[cvapcolumn].sum() - block[cvapcolumn].sum(), 0)

    # Fill NaNs with 0 and drop unnecessary columns.
    block = block.fillna(0)
    block = block.drop([&#34;tmp&#34;, &#34;BGVAP20&#34;], axis=1)

    # Return!
    return block</code></pre>
</details>
</dd>
<dt id="gerrytools.data.fetchgeometries"><code class="name flex">
<span>def <span class="ident">fetchgeometries</span></span>(<span>state, geometry) ‑> geopandas.geodataframe.GeoDataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Fetches the 2010 Census geometries on which ACS data are reported.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>State</code></dt>
<dd>The <code>us.State</code> for which CVAP will be estimated.</dd>
<dt><strong><code>geometry10</code></strong> :&ensp;<code>str</code></dt>
<dd>Level of geometry we're fetching. Accepted values are
<code>"tract"</code> and <code>"block group"</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A <code>GeoDataFrame</code> of 2010 geometries.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fetchgeometries(state, geometry) -&gt; gpd.GeoDataFrame:
    &#34;&#34;&#34;
    Fetches the 2010 Census geometries on which ACS data are reported.

    Args:
        state (State): The `us.State` for which CVAP will be estimated.
        geometry10 (str): Level of geometry we&#39;re fetching. Accepted values are
            `&#34;tract&#34;` and `&#34;block group&#34;`.

    Returns:
        A `GeoDataFrame` of 2010 geometries.
    &#34;&#34;&#34;
    # Get a Census locator for the provided State by replacing spaces in its name
    # with underscores (should they exist).
    clocator = state.name.replace(&#34; &#34;, &#34;_&#34;)

    # Validate geometry level indicators.
    if geometry not in {&#34;block group&#34;, &#34;tract&#34;, &#34;block&#34;}:
        raise ValueError(f&#34;Geometry level {geometry} not supported; aborting.&#34;)

    if geometry == &#34;block group&#34;:
        geometry = &#34;bg&#34;
    if geometry == &#34;block&#34;:
        geometry = &#34;tabblock&#34;

    # Construct the Census URL.
    fips = state.fips
    head = &#34;https://www2.census.gov/geo/pvs/tiger2010st/&#34;
    tail = f&#34;{fips}_{clocator}/{fips}/tl_2010_{fips}_{geometry}10.zip&#34;
    url = head + tail

    # Download, extract, and return the geometries from the URL.
    return gpd.read_file(url)</code></pre>
</details>
</dd>
<dt id="gerrytools.data.ids"><code class="name flex">
<span>def <span class="ident">ids</span></span>(<span>state)</span>
</code></dt>
<dd>
<div class="desc"><p>URL for accessing districtr identifiers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>Name of the state (e.g. <code>"wisconsin"</code>) for which we're retrieving
districtr identifiers.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>String with the appropriate URL.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ids(state):
    &#34;&#34;&#34;
    URL for accessing districtr identifiers.

    Args:
        state: Name of the state (e.g. `&#34;wisconsin&#34;`) for which we&#39;re retrieving
            districtr identifiers.

    Returns:
        String with the appropriate URL.
    &#34;&#34;&#34;
    # If we&#39;re in michigan, then we use the &#39;beta&#39; pipeline instead of the &#39;prod&#39;
    # one.
    pipeline = &#34;beta&#34; if state == us.states.MI else &#34;prod&#34;
    if state == us.states.MI:
        prefix = &#34;https://o1siz7rw0c.execute-api.us-east-2.amazonaws.com&#34;
    else:
        prefix = &#34;https://k61e3cz2ni.execute-api.us-east-2.amazonaws.com&#34;

    return f&#34;{prefix}/{pipeline}/submissions/districtr-ids/{state.name.lower()}&#34;</code></pre>
</details>
</dd>
<dt id="gerrytools.data.one"><code class="name flex">
<span>def <span class="ident">one</span></span>(<span>identifier)</span>
</code></dt>
<dd>
<div class="desc"><p>URL for accessing an individual districtr plan.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>identifier</code></strong></dt>
<dd>districtr identifier.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>String with the appropriate URL.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def one(identifier):
    &#34;&#34;&#34;
    URL for accessing an individual districtr plan.

    Args:
        identifier: districtr identifier.

    Returns:
        String with the appropriate URL.
    &#34;&#34;&#34;
    return f&#34;https://districtr.org/.netlify/functions/planRead?id={identifier}&#34;</code></pre>
</details>
</dd>
<dt id="gerrytools.data.remap"><code class="name flex">
<span>def <span class="ident">remap</span></span>(<span>plans, unitmaps, popmap=None) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Re-maps assignments to the specified set of units.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>plans</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>The Pandas DataFrame produced by <code><a title="gerrytools.data.tabularized" href="#gerrytools.data.tabularized">tabularized()</a></code>.</dd>
<dt><strong><code>unitmaps</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary whose keys are unit types appearing in the
<code>unitsType</code> column, and whose values are dictionaries mapping unique
identifiers of one set of geometries to unique identifiers (or lists
of unique identifiers) of another set of geometries; these correspond
to mappings generated by <code>unitmap()</code> and the inverse mapping generated
by <code>invert()</code>.</dd>
<dt><strong><code>popmap</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>A mapping from unit unique identifiers to
population values. Only applies when we are mapping from smaller
units to larger ones.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remap(plans, unitmaps, popmap=None) -&gt; Callable:
    &#34;&#34;&#34;
    Re-maps assignments to the specified set of units.

    Args:
        plans (DataFrame): The Pandas DataFrame produced by ``tabularized()``.
        unitmaps (dict): A dictionary whose keys are unit types appearing in the
            `unitsType` column, and whose values are dictionaries mapping unique
            identifiers of one set of geometries to unique identifiers (or lists
            of unique identifiers) of another set of geometries; these correspond
            to mappings generated by `unitmap()` and the inverse mapping generated
            by `invert()`.
        popmap (dict, optional): A mapping from unit unique identifiers to
            population values. Only applies when we are mapping from smaller
            units to larger ones.

    Returns:
        A function
    &#34;&#34;&#34;
    def _(row):
        # Get the assignment for the row.
        assignment = ast.literal_eval(row[&#34;plan&#34;])

        # Attempt to get the type of units specified by the row; if we can&#39;t – i.e.
        # the user didn&#39;t specify a unit mapping corresponding to that unit type
        # in `unitmaps` – we leave the assignment alone and warn the user.
        try:
            unitsType = row[&#34;units&#34;]
            unitmap = unitmaps[unitsType]
        except BaseException:
            print(f&#34;No unit mapping provided for {row[&#39;units&#39;]}; skipping.&#34;)
            return assignment

        # What kind of mapping do we have? If `mapping` is from a single key
        # to a single value, then we&#39;re mapping units one-to-one (e.g. block IDs
        # to VTD IDs); otherwise, we&#39;re mapping units one-to-many (e.g. VTD IDs
        # to blocks). If `mapping` is of the former type, then it&#39;s possible that
        # some larger units may comprise smaller units in multiple districts. If
        # this is the case, then we assign larger units to the district in which
        # most of the larger unit&#39;s population resides; otherwise, we simply
        # assign all smaller units to whichever district the larger unit&#39;s in.
        firstvalue = next(iter(unitmap.values()))
        unitmapdirection = &#34;down&#34;

        # Mark which kind of mapping we have.
        if isinstance(firstvalue, list):
            unitmapdirection = &#34;down&#34;
        else:
            unitmapdirection = &#34;up&#34;

        # Now, based on the mapping type, return the appropriate mapping.
        if unitmapdirection == &#34;down&#34;:
            return _down(unitmap, assignment)
        return _up(unitmap, popmap, assignment)

    plans[&#34;plan&#34;] = plans.apply(_, axis=1)
    return plans</code></pre>
</details>
</dd>
<dt id="gerrytools.data.submissions"><code class="name flex">
<span>def <span class="ident">submissions</span></span>(<span>state, sample=None) ‑> List[<a title="gerrytools.data.fetch.Submission" href="fetch.html#gerrytools.data.fetch.Submission">Submission</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves raw districtr objects; this includes both plan- and COI-based
submissions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>State</code></dt>
<dd><code>us.State</code> object (e.g. <code>us.states.WI</code>).</dd>
<dt><strong><code>sample</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of sample plans to retrieve.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A list of <code>Submissions</code>, either to be interpreted raw or tabularized.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def submissions(state, sample=None) -&gt; List[Submission]:
    &#34;&#34;&#34;
    Retrieves raw districtr objects; this includes both plan- and COI-based
    submissions.

    Args:
        state (State): `us.State` object (e.g. `us.states.WI`).
        sample (int, optional): The number of sample plans to retrieve.

    Returns:
        A list of `Submissions`, either to be interpreted raw or tabularized.
    &#34;&#34;&#34;
    # Get the appropriate URL and send the request. Made some basic ASCII art with
    # the second three variable names... it&#39;s like the request is loading letter
    # by letter.
    url = ids(state)
    __w = requests.get(url).text
    _aw = json.loads(__w)[&#34;ids&#34;]
    raw = _aw[:sample] if sample else _aw

    # Create `Submission` objects for each of the retrieved objects. Getting the
    # individual plans is the bottleneck here, and unfortunately we can&#39;t retrieve
    # them in bulk (... or can we?).
    submissions = []
    for entity in raw:
        # Retrieve the required data points.
        identifier = parse_id(entity[&#34;link&#34;], df=False)
        districtr = individual(identifier)

        # Force all plan keys and values to strings.
        try:
            plan = {
                str(k): str(v) if not isinstance(v, list) else str(v[0])
                for k, v in districtr[&#34;plan&#34;][&#34;assignment&#34;].items()
            }
            units = districtr[&#34;plan&#34;][&#34;units&#34;][&#34;name&#34;]
            unitsType = districtr[&#34;plan&#34;][&#34;units&#34;][&#34;unitType&#34;]
            tileset = districtr[&#34;plan&#34;][&#34;units&#34;][&#34;tilesets&#34;][0][&#34;sourceLayer&#34;]

            # Create a new Submission.
            submissions.append(Submission(
                link=entity[&#34;link&#34;],
                id=identifier,
                plan=plan,
                units=units,
                unitsType=unitsType,
                tileset=tileset,
                type=entity[&#34;type&#34;]
            ))
        except BaseException:
            pass

    return submissions</code></pre>
</details>
</dd>
<dt id="gerrytools.data.tabularized"><code class="name flex">
<span>def <span class="ident">tabularized</span></span>(<span>state, submissions) ‑> Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns districtr submission information in a tabular format.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>State</code></dt>
<dd><code>us.State</code> object (e.g. <code>us.states.WI</code>).</dd>
<dt><strong><code>submissions</code></strong> :&ensp;<code>list</code></dt>
<dd>List of <code><a title="gerrytools.data.Submission" href="#gerrytools.data.Submission">Submission</a></code> objects returned from a call to
<code><a title="gerrytools.data.submissions" href="#gerrytools.data.submissions">submissions()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Three dataframes corresponding to plan-based submissions, COI-based
submissions, and written submissions to the provided state.</p>
<h2 id="example">Example</h2>
<p>Prototypical example usage.</p>
<pre><code>import us
from gerrytools.retrieve import submissions, tabularized

# Set the state.
state = us.states.WI

# Retrieve the raw districtr submissions, then tabularize them.
subs = submissions(state)
plans, cois, written = tabularized(state, subs)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tabularized(state, submissions) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    &#34;&#34;&#34;
    Returns districtr submission information in a tabular format.

    Args:
        state (State): `us.State` object (e.g. `us.states.WI`).
        submissions (list): List of `Submission` objects returned from a call to
            `submissions`.

    Returns:
        Three dataframes corresponding to plan-based submissions, COI-based
        submissions, and written submissions to the provided state.

    Example:
        Prototypical example usage.

            import us
            from gerrytools.retrieve import submissions, tabularized

            # Set the state.
            state = us.states.WI

            # Retrieve the raw districtr submissions, then tabularize them.
            subs = submissions(state)
            plans, cois, written = tabularized(state, subs)

    &#34;&#34;&#34;
    # Sort submissions. (Not sure why this is necessary? Holdover from previous
    # fetching code.)
    submissions = list(sorted(submissions, key=lambda s: s.id))

    # Categorize into three categories: plan submissions, COI submissions, and
    # written submissions (which are ignored as they don&#39;t appear in the list
    # of submissions).
    _plans = [s.dict() for s in submissions if s.type == &#34;plan&#34;]
    _cois = [s.dict() for s in submissions if s.type == &#34;coi&#34;]

    # Create preliminary dataframes so we can do safe `merge`s rather than rely
    # explicitly on sorting; this also allows us to specify a sample size if
    # we&#39;re only looking to sample a specific number of plans.
    subset_plans = pd.DataFrame.from_records(_plans)
    subset_cois = pd.DataFrame.from_records(_cois)

    # Get appropriate URLs and create dataframes.
    plans_url = csvs(state)
    cois_url = csvs(state, ptype=&#34;coi&#34;)
    written_url = csvs(state, ptype=&#34;written&#34;)

    plans = as_dataframe(plans_url)
    cois = as_dataframe(cois_url)
    writtens = as_dataframe(written_url)

    # Adjust column contents for the plan and COI dataframes.
    for universe in [plans, cois]:
        # Adjust the `link` column type and create an `id` column from it.
        universe[&#34;link&#34;] = universe[&#34;link&#34;].astype(str)
        universe[&#34;id&#34;] = parse_id(universe[&#34;link&#34;])

    # Adjust column contents for all dataframes.
    for df in [plans, cois, writtens]:
        df[&#34;datetime&#34;] = parse_datetime(df[&#34;datetime&#34;])

    # Add the retrieved plan data to the dataframes *if the subset dataframes
    # contain items*.
    if not subset_plans.empty:
        plans = plans.merge(subset_plans, on=&#34;id&#34;)
    else:
        plans = pd.DataFrame()
    if not subset_cois.empty:
        cois = cois.merge(subset_cois, on=&#34;id&#34;)
    else:
        cois = pd.DataFrame()

    # Drop bad columns and rename. Not sure why we have to `inplace` things here,
    # but... fine.
    for df in [plans, cois]:
        if not df.empty:
            # Remove columns we don&#39;t necessarily care about.
            for col in [&#34;type_x&#34;, &#34;link_x&#34;, &#34;coalition&#34;]:
                if col in list(df):
                    df.drop(col, axis=1, inplace=True)

            # Rename the columns we do care about.
            df.rename({&#34;type_y&#34;: &#34;type&#34;, &#34;link_y&#34;: &#34;link&#34;}, axis=1, inplace=True)

    return plans, cois, writtens</code></pre>
</details>
</dd>
<dt id="gerrytools.data.variables"><code class="name flex">
<span>def <span class="ident">variables</span></span>(<span>table) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Produces variable names for the 2020 Census PL94-171 tables. Variables are
determined from patterns apparent in PL94 variable <a href="https://tinyurl.com/2s3btptn">lists for tables P1 through
P4</a>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>table</code></strong> :&ensp;<code>string</code></dt>
<dd>The table for which we're generating variables.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A dictionary mapping Census variable codes to human-readable ones.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def variables(table) -&gt; dict:
    &#34;&#34;&#34;
    Produces variable names for the 2020 Census PL94-171 tables. Variables are
    determined from patterns apparent in PL94 variable [lists for tables P1 through
    P4](https://tinyurl.com/2s3btptn).

    Args:
        table (string): The table for which we&#39;re generating variables.

    Returns:
        A dictionary mapping Census variable codes to human-readable ones.
    &#34;&#34;&#34;
    # List the categories of Census variables and find the combinations in the
    # correct order. This *should* be the original order in which they&#39;re listed,
    # but these have been spot-checked to verify their correctness. These names
    # are also modified based on the table passed; for example, if the table
    # passed is P2 or P4, we prepend an &#34;NH&#34; to the beginning, as these columns
    # are explicitly non-hispanic people. If the table passed is P3 or P4, we
    # append a &#34;VAP&#34; to the end to signify these are people of voting age;
    # otherwise, we add &#34;POP.&#34;
    categories = [&#34;WHITE&#34;, &#34;BLACK&#34;, &#34;AMIN&#34;, &#34;ASIAN&#34;, &#34;NHPI&#34;, &#34;OTH&#34;]
    prefix = &#34;NH&#34; if table in {&#34;P2&#34;, &#34;P4&#34;} else &#34;&#34;
    suffix = &#34;VAP&#34; if table in {&#34;P3&#34;, &#34;P4&#34;} else &#34;POP&#34;
    combos = list(pd.core.common.flatten(
        [
            prefix + &#34;&#34;.join(list(combo)) + suffix + &#34;20&#34;
            for i in range(1, len(categories) + 1)
            for combo in list(combinations(categories, i))
        ]
    ))

    # Now, for each of the combinations, we map the appropriate variable name to
    # the descriptor. Each of these tranches should have a width of 6 choose i,
    # where i is the number of categories in the combination. For example, the
    # second tranch (from 13 to 27) has width 15, as 6C2=15.
    if table in {&#34;P1&#34;, &#34;P3&#34;}:
        tranches = [(3, 8), (11, 25), (27, 46), (48, 62), (64, 69), (71, 71)]
    else:
        tranches = [(5, 10), (13, 27), (29, 48), (50, 64), (66, 71), (73, 73)]

    # Create variable numbers.
    numbers = list(pd.core.common.flatten([list(range(i, j + 1)) for i, j in tranches]))

    # Edit these for specific tables. For example, in tables P2 and P3, we want
    # to get the total Hispanic population and the total population.
    if table in {&#34;P2&#34;, &#34;P4&#34;}:
        numbers = [1, 2] + numbers
        hcol = &#34;HVAP20&#34; if table == &#34;P4&#34; else &#34;HPOP20&#34;
        tcol = &#34;VAP20&#34; if table == &#34;P4&#34; else &#34;TOTPOP20&#34;
        combos = [tcol, hcol] + combos
    else:
        numbers = [1] + numbers
        tcol = &#34;VAP20&#34; if table in {&#34;P3&#34;, &#34;P4&#34;} else &#34;TOTPOP20&#34;
        combos = [tcol] + combos

    # Create the variable names and zip the names together with the combinations.
    names = [f&#34;{table}_{str(n).zfill(3)}N&#34; for n in numbers]
    return dict(zip(names, combos))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gerrytools.data.AssignmentCompressor"><code class="flex name class">
<span>class <span class="ident">AssignmentCompressor</span></span>
<span>(</span><span>identifiers, window=10, location='compressed.ac')</span>
</code></dt>
<dd>
<div class="desc"><p>A class for compressing and decompressing lots of assignments very, very
quickly. Intended for use with <code>jsonlines</code>-like libraries (where assignments
are read in line-by-line) or for network requests (where assignments are
retrieved one-by-one). When decompressing, yields <code>dict</code>s where keys are
in sorted order.</p>
<p>The compression schema considers the set of unique identifiers, imposes an
ordering (lexicographic order) on the identifiers, and matches the assignment
to that ordering. We assign all unassigned units to <code>"-1"</code> and, once the
default cache size is hit (or assignments are no longer being read in),
compress all assignments in the cache. Assignments are read in and out in
the same order, and the keys for each assignment are in the same order.</p>
<h2 id="example">Example</h2>
<p>To compress assignments, we need a set of unique identifiers such that
each identifier maps one geometric unit to one district.</p>
<pre><code>...

geoids = blocks["GEOID20"].astype(str)
ac = AssignmentCompressor(geoids, location="compressed-assignments.ac")

with ac as compressor:
    for assignment in assignments:
        # Here, ensure that all assignments have string keys and
        # string values; also ensure that an assignment's keys are
        # a subset of geoids (or whatever IDs you're passing).
        compressor.compress(assignment)

...
</code></pre>
<p>To decompress assignments, we again must have a set of unique geometric
identifiers which match the assignments. We can then iterate over the
decompressed assignments as they're read out of the file.</p>
<pre><code>...

geoids = blocks["GEOID20"].astype(str)
ac = AssignmentCompressor(geoids, location="compressed-assignments.ac")

for assignment in ac.decompress():
    &lt;do whatever!&gt;

...
</code></pre>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>DISTRICT_DELIMITER</code></strong> :&ensp;<code>bytes</code></dt>
<dd>A bytestring which separates district
identifiers in an assignment.</dd>
<dt><strong><code>ASSIGNMENT_DELIMITER</code></strong> :&ensp;<code>bytes</code></dt>
<dd>A bytestring which separates assignments from
each other.</dd>
<dt><strong><code>CHUNK_DELIMITER</code></strong> :&ensp;<code>bytes</code></dt>
<dd>A bytestring which separates assignment chunks
from each other.</dd>
<dt><strong><code>CHUNK_SIZE</code></strong> :&ensp;<code>int</code></dt>
<dd>Default number of bytes read in from the IO stream at each
step.</dd>
<dt><strong><code>ENCODING</code></strong> :&ensp;<code>str</code></dt>
<dd>Default string encoding style.</dd>
<dt><strong><code>identifiers</code></strong></dt>
<dd>A sortable, iterable collection of unique items corresponding
to geographic identifiers.</dd>
<dt><strong><code>compressed</code></strong></dt>
<dd>A pandas <code>Index</code> containing the identifiers; this is used
to quickly perform vectorized identifier matchings, rather than using
traditional iterative methods.</dd>
<dt><strong><code>cache</code></strong></dt>
<dd>Collection of assignments to be compressed. Assignments are loaded
into the cache every time the <code>.compress()</code> method is called, and
is cleared whenever the length of the cache exceeds the window width.</dd>
<dt><strong><code>window</code></strong></dt>
<dd>Maximum cache length before the cache is compressed, written to
file, and emptied.</dd>
<dt><strong><code>default</code></strong></dt>
<dd>The default assignment which is updated each time an assignment
is passed to the compressor.</dd>
<dt><strong><code>location</code></strong></dt>
<dd>The place to which compressed data is written or read.</dd>
</dl>
<p>Creates <code><a title="gerrytools.data.AssignmentCompressor" href="#gerrytools.data.AssignmentCompressor">AssignmentCompressor</a></code> instance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>identifiers</code></strong> :&ensp;<code>list</code></dt>
<dd>An iterable collection of string identifiers; any
assignment's keys must be a subset of <code>identifiers</code>.</dd>
<dt><strong><code>window</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>A positive integer representing the cache
window size. Defaults to cache.</dd>
<dt><strong><code>location</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to the compressed resource (read
or write). Defaults to <code>compressed.ac</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AssignmentCompressor:
    &#34;&#34;&#34;
    A class for compressing and decompressing lots of assignments very, very
    quickly. Intended for use with ``jsonlines``-like libraries (where assignments
    are read in line-by-line) or for network requests (where assignments are
    retrieved one-by-one). When decompressing, yields ``dict``s where keys are
    in sorted order.

    The compression schema considers the set of unique identifiers, imposes an
    ordering (lexicographic order) on the identifiers, and matches the assignment
    to that ordering. We assign all unassigned units to ``&#34;-1&#34;`` and, once the
    default cache size is hit (or assignments are no longer being read in),
    compress all assignments in the cache. Assignments are read in and out in
    the same order, and the keys for each assignment are in the same order.

    Example:
        To compress assignments, we need a set of unique identifiers such that
        each identifier maps one geometric unit to one district.

            ...

            geoids = blocks[&#34;GEOID20&#34;].astype(str)
            ac = AssignmentCompressor(geoids, location=&#34;compressed-assignments.ac&#34;)

            with ac as compressor:
                for assignment in assignments:
                    # Here, ensure that all assignments have string keys and
                    # string values; also ensure that an assignment&#39;s keys are
                    # a subset of geoids (or whatever IDs you&#39;re passing).
                    compressor.compress(assignment)

            ...

        To decompress assignments, we again must have a set of unique geometric
        identifiers which match the assignments. We can then iterate over the
        decompressed assignments as they&#39;re read out of the file.

            ...

            geoids = blocks[&#34;GEOID20&#34;].astype(str)
            ac = AssignmentCompressor(geoids, location=&#34;compressed-assignments.ac&#34;)

            for assignment in ac.decompress():
                &lt;do whatever!&gt;

            ...

    Attributes:
        DISTRICT_DELIMITER (bytes): A bytestring which separates district
            identifiers in an assignment.
        ASSIGNMENT_DELIMITER (bytes): A bytestring which separates assignments from
            each other.
        CHUNK_DELIMITER (bytes): A bytestring which separates assignment chunks
            from each other.
        CHUNK_SIZE (int): Default number of bytes read in from the IO stream at each
            step.
        ENCODING (str): Default string encoding style.
        identifiers: A sortable, iterable collection of unique items corresponding
            to geographic identifiers.
        compressed: A pandas `Index` containing the identifiers; this is used
            to quickly perform vectorized identifier matchings, rather than using
            traditional iterative methods.
        cache: Collection of assignments to be compressed. Assignments are loaded
            into the cache every time the ``.compress()`` method is called, and
            is cleared whenever the length of the cache exceeds the window width.
        window: Maximum cache length before the cache is compressed, written to
            file, and emptied.
        default: The default assignment which is updated each time an assignment
            is passed to the compressor.
        location: The place to which compressed data is written or read.
    &#34;&#34;&#34;

    def __init__(self, identifiers, window=10, location=&#34;compressed.ac&#34;):
        &#34;&#34;&#34;
        Creates `AssignmentCompressor` instance.

        Args:
            identifiers (list): An iterable collection of string identifiers; any
                assignment&#39;s keys must be a subset of `identifiers`.
            window (int, optional): A positive integer representing the cache
                window size. Defaults to cache.
            location (str, optional): The path to the compressed resource (read
                or write). Defaults to `compressed.ac`.
        &#34;&#34;&#34;
        self.identifiers = SortedList(identifiers)
        self.default = frozenset(zip(self.identifiers, [&#34;-1&#34;] * len(self.identifiers)))
        self.cache = []
        self.location = location

        # Error to users if the window is nonexistent.
        if not isinstance(window, int) or window &lt;= 0:
            raise ValueError(&#34;Cache window width must be a positive integer.&#34;)

        self.window = window

        self.DISTRICT_DELIMITER = b&#34;,&#34;
        self.ASSIGNMENT_DELIMITER = b&#34;&lt;&lt;&lt;*&gt;&gt;&gt;&#34;
        self.CHUNK_DELIMITER = b&#34;(((*)))&#34;
        self.CHUNK_SIZE = 16384
        self.ENCODING = &#34;raw_unicode_escape&#34;

    def match(self, assignment) -&gt; SortedDict:
        &#34;&#34;&#34;
        Matches an assignment to an index (the set of geometric identifiers)
        and returns a `SortedDict`.

        Args:
            assignment (dict): Dictionary which matches geometric identifiers to
                districts. All keys and values in this dictionary must be strings.

        Returns:
            A `SortedDict` with identifiers matched ti district assignments.s

        &#34;&#34;&#34;
        # Create a dictionary which maps identifiers to `-1`, and update our
        # dictionary with assignment values.
        indexer = SortedDict(self.default)
        indexer.update(assignment)

        return indexer

    def __enter__(self):
        &#34;&#34;&#34;
        A simple context-management method. Allows the user to use `with`
        statements when compressing stuff so we don&#39;t have to worry about the
        user specifying the last item they&#39;ll be compressing.
        &#34;&#34;&#34;
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        &#34;&#34;&#34;
        Teardown. Once we&#39;ve exited the `with` statement (i.e. the user&#39;s all done
        feeding items to the compressor) we can force the remaining items to be
        compressed and written to file.
        &#34;&#34;&#34;
        if self.cache:
            self._compress(force=True)

    def compress_all(self, assignments):
        &#34;&#34;&#34;
        Compresses all assignments in `assignments`.

        Args:
            assignments (list): List of dictionaries which match geometric identifiers
                to districts. All keys and values in these dictionaries must be
                strings.
        &#34;&#34;&#34;
        self.window = len(assignments)

        with self as ac:
            for assignment in assignments:
                ac.compress(assignment)

    def compress(self, assignment):
        &#34;&#34;&#34;
        Compresses the assignment `assignment` using `zlib`.

        Args:
            assignment (dict): Dictionary which matches geometric identifiers to districts.
                All keys and values in this dictionary must be strings.
        &#34;&#34;&#34;
        # If the user provides an empty assignment or the assignment&#39;s keys aren&#39;t
        # a subset of `identifiers`, warn the user and skip the assignment.
        skip = False

        if not assignment:
            skip = True
            print(&#34;`assignment` is empty; skipping.&#34;)

        if not set(assignment.keys()).issubset(self.identifiers):
            skip = True
            print(
                &#34;`assignment`&#39;s keys are not a subset of `identifiers`; skipping. &#34; +
                &#34;Please ensure that all keys and values in `assignment` are strings.&#34;,
            )

        # Join the things on the district separator, encode the whole thing, and
        # encode according to the default encoding.
        if not skip:
            indexed = self.match(assignment)
            sep = self.DISTRICT_DELIMITER.decode()
            encoded = bytes(sep.join(indexed.values()).encode(self.ENCODING))

            # Compress.
            self.cache += [encoded]
            self._compress()

    def _compress(self, force=False):
        &#34;&#34;&#34;
        Private method which actually does the compression.

        Args:
            force: If truthy, then the length of the cache is ignored, the data
                in the cache are compressed, and the compressed data is written
                to file. ``force`` is truthy when teardown logic is entered
                (i.e. after ``__exit()__`` is called).
        &#34;&#34;&#34;
        # Check to see if the cache is full. If so, compress the data, write to
        # file, and reset the cache.
        if len(self.cache) == self.window or force:
            with open(self.location, &#34;ab&#34;) as writer:
                # Write compressed data to file.
                compressed = zlib.compress(
                    bytes(self.ASSIGNMENT_DELIMITER.join(self.cache))
                )
                writer.write(compressed)

                # We only forcibly write the chunk separator to file if we&#39;ve
                # entered teardown logic and the cache *is not empty*. If the
                # cache is empty when we&#39;re entering teardown logic, that means
                # (number of assignments compressed) == (window width), in which
                # case we&#39;ve reached the end of compression and should not write
                # a separator; doing so will produce an empty bytestring (which,
                # in turn, produces a dictionary with one key, corresponding to
                # a null assignment).
                if not force:
                    writer.write(self.CHUNK_DELIMITER)

            # Reset the cache.
            self.cache = []

    def decompress(self):
        &#34;&#34;&#34;
        Decompresses the data at ``location``. A generator which ``yield``s
        assignments.

        Yields:
            Decompressed assignment dictionaries.
        &#34;&#34;&#34;
        # Open the compressed file. Then we read it in chunks, loading until
        # we hit our separator or until the end of the file.
        with open(self.location, &#34;rb&#34;) as _compressed_fin:
            for chunk in self._chunk(_compressed_fin):
                if not chunk:
                    break
                for assignment in self._decompress(chunk):
                    yield assignment

    def _chunk(self, stream):
        &#34;&#34;&#34;
        Private method for reading chunks from file without holding the entire
        file in memory. A generator for decompressed assignments.

        Args:
            stream: A ``BytesIO`` instance from which data is read.

        Yields:
            Buffered, compressed bytes to be fed into the decompressor.
        &#34;&#34;&#34;
        # Create a buffer.
        _buffer = []

        # Read until we hit the end of the file `yield`ing each chunk as we go.
        while True:
            # Read in a chunk of data.
            chunk = stream.read(self.CHUNK_SIZE)
            _buffer.append(chunk)

            # Check if the chunk has our delimiter in it. If it contains our
            # delimiter, then the buffer *up to the delimiter* contains compressed
            # assignments; this should be `yield`ed and decompressed. We only
            # want to get the part before the delimiter for decompression, but
            # retain the rest.
            if self.CHUNK_DELIMITER in chunk:
                _buffered_bytes = b&#34;&#34;.join(_buffer)
                part, _buffer_first = _buffered_bytes.split(self.CHUNK_DELIMITER, 1)
                _buffer = [_buffer_first]
                yield part

            # If the chunk&#39;s empty, `yield` the remaining buffer and return.
            if not chunk:
                yield b&#34;&#34;.join(_buffer)
                break

    def _decompress(self, chunk) -&gt; List[dict]:
        &#34;&#34;&#34;
        Private method which decompresses assignments.

        Args:
            chunk: Compressed, byte-encoded data representing ``window``
                assignments.

        Returns:
            List of decompressed assignment objects.
        &#34;&#34;&#34;
        # Decompress the chunk and split it on our delimiter.
        decompressed = zlib.decompress(chunk)
        decompressed_parts = decompressed.split(self.ASSIGNMENT_DELIMITER)

        # For each of the parts, decode the bytes, make them into lists, and
        # match them to GEOIDs.
        decoded_parts = [part.decode() for part in decompressed_parts]
        split_parts = [part.split(self.DISTRICT_DELIMITER.decode()) for part in decoded_parts]
        indexed_parts = [dict(zip(self.identifiers, part)) for part in split_parts]

        return indexed_parts</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="gerrytools.data.AssignmentCompressor.compress"><code class="name flex">
<span>def <span class="ident">compress</span></span>(<span>self, assignment)</span>
</code></dt>
<dd>
<div class="desc"><p>Compresses the assignment <code>assignment</code> using <code>zlib</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>assignment</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary which matches geometric identifiers to districts.
All keys and values in this dictionary must be strings.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compress(self, assignment):
    &#34;&#34;&#34;
    Compresses the assignment `assignment` using `zlib`.

    Args:
        assignment (dict): Dictionary which matches geometric identifiers to districts.
            All keys and values in this dictionary must be strings.
    &#34;&#34;&#34;
    # If the user provides an empty assignment or the assignment&#39;s keys aren&#39;t
    # a subset of `identifiers`, warn the user and skip the assignment.
    skip = False

    if not assignment:
        skip = True
        print(&#34;`assignment` is empty; skipping.&#34;)

    if not set(assignment.keys()).issubset(self.identifiers):
        skip = True
        print(
            &#34;`assignment`&#39;s keys are not a subset of `identifiers`; skipping. &#34; +
            &#34;Please ensure that all keys and values in `assignment` are strings.&#34;,
        )

    # Join the things on the district separator, encode the whole thing, and
    # encode according to the default encoding.
    if not skip:
        indexed = self.match(assignment)
        sep = self.DISTRICT_DELIMITER.decode()
        encoded = bytes(sep.join(indexed.values()).encode(self.ENCODING))

        # Compress.
        self.cache += [encoded]
        self._compress()</code></pre>
</details>
</dd>
<dt id="gerrytools.data.AssignmentCompressor.compress_all"><code class="name flex">
<span>def <span class="ident">compress_all</span></span>(<span>self, assignments)</span>
</code></dt>
<dd>
<div class="desc"><p>Compresses all assignments in <code>assignments</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>assignments</code></strong> :&ensp;<code>list</code></dt>
<dd>List of dictionaries which match geometric identifiers
to districts. All keys and values in these dictionaries must be
strings.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compress_all(self, assignments):
    &#34;&#34;&#34;
    Compresses all assignments in `assignments`.

    Args:
        assignments (list): List of dictionaries which match geometric identifiers
            to districts. All keys and values in these dictionaries must be
            strings.
    &#34;&#34;&#34;
    self.window = len(assignments)

    with self as ac:
        for assignment in assignments:
            ac.compress(assignment)</code></pre>
</details>
</dd>
<dt id="gerrytools.data.AssignmentCompressor.decompress"><code class="name flex">
<span>def <span class="ident">decompress</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Decompresses the data at <code>location</code>. A generator which <code>yield</code>s
assignments.</p>
<h2 id="yields">Yields</h2>
<p>Decompressed assignment dictionaries.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decompress(self):
    &#34;&#34;&#34;
    Decompresses the data at ``location``. A generator which ``yield``s
    assignments.

    Yields:
        Decompressed assignment dictionaries.
    &#34;&#34;&#34;
    # Open the compressed file. Then we read it in chunks, loading until
    # we hit our separator or until the end of the file.
    with open(self.location, &#34;rb&#34;) as _compressed_fin:
        for chunk in self._chunk(_compressed_fin):
            if not chunk:
                break
            for assignment in self._decompress(chunk):
                yield assignment</code></pre>
</details>
</dd>
<dt id="gerrytools.data.AssignmentCompressor.match"><code class="name flex">
<span>def <span class="ident">match</span></span>(<span>self, assignment) ‑> sortedcontainers.sorteddict.SortedDict</span>
</code></dt>
<dd>
<div class="desc"><p>Matches an assignment to an index (the set of geometric identifiers)
and returns a <code>SortedDict</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>assignment</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary which matches geometric identifiers to
districts. All keys and values in this dictionary must be strings.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A <code>SortedDict</code> with identifiers matched ti district assignments.s</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def match(self, assignment) -&gt; SortedDict:
    &#34;&#34;&#34;
    Matches an assignment to an index (the set of geometric identifiers)
    and returns a `SortedDict`.

    Args:
        assignment (dict): Dictionary which matches geometric identifiers to
            districts. All keys and values in this dictionary must be strings.

    Returns:
        A `SortedDict` with identifiers matched ti district assignments.s

    &#34;&#34;&#34;
    # Create a dictionary which maps identifiers to `-1`, and update our
    # dictionary with assignment values.
    indexer = SortedDict(self.default)
    indexer.update(assignment)

    return indexer</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="gerrytools.data.Submission"><code class="flex name class">
<span>class <span class="ident">Submission</span></span>
<span>(</span><span>**data: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Provides a base model for data retrieved from districtr. Allows us to use
dot notation when accessing properties rather than dict notation.</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Submission(BaseModel):
    &#34;&#34;&#34;
    Provides a base model for data retrieved from districtr. Allows us to use
    dot notation when accessing properties rather than dict notation.
    &#34;&#34;&#34;
    link: str
    &#34;&#34;&#34;A districtr URL.&#34;&#34;&#34;
    plan: dict
    &#34;&#34;&#34;districtr plan object.&#34;&#34;&#34;
    id: str
    &#34;&#34;&#34;districtr identifier.&#34;&#34;&#34;
    units: str
    &#34;&#34;&#34;Unit identifier (e.g. `GEOID`).&#34;&#34;&#34;
    unitsType: str
    &#34;&#34;&#34;Unit type (e.g. `blocks20`, `blockgroup`, etc.)&#34;&#34;&#34;
    tileset: str
    &#34;&#34;&#34;Mapbox tileset URL.&#34;&#34;&#34;
    type: str
    &#34;&#34;&#34;Not sure.&#34;&#34;&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="gerrytools.data.Submission.id"><code class="name">var <span class="ident">id</span> : str</code></dt>
<dd>
<div class="desc"><p>districtr identifier.</p></div>
</dd>
<dt id="gerrytools.data.Submission.link"><code class="name">var <span class="ident">link</span> : str</code></dt>
<dd>
<div class="desc"><p>A districtr URL.</p></div>
</dd>
<dt id="gerrytools.data.Submission.plan"><code class="name">var <span class="ident">plan</span> : dict</code></dt>
<dd>
<div class="desc"><p>districtr plan object.</p></div>
</dd>
<dt id="gerrytools.data.Submission.tileset"><code class="name">var <span class="ident">tileset</span> : str</code></dt>
<dd>
<div class="desc"><p>Mapbox tileset URL.</p></div>
</dd>
<dt id="gerrytools.data.Submission.type"><code class="name">var <span class="ident">type</span> : str</code></dt>
<dd>
<div class="desc"><p>Not sure.</p></div>
</dd>
<dt id="gerrytools.data.Submission.units"><code class="name">var <span class="ident">units</span> : str</code></dt>
<dd>
<div class="desc"><p>Unit identifier (e.g. <code>GEOID</code>).</p></div>
</dd>
<dt id="gerrytools.data.Submission.unitsType"><code class="name">var <span class="ident">unitsType</span> : str</code></dt>
<dd>
<div class="desc"><p>Unit type (e.g. <code>blocks20</code>, <code>blockgroup</code>, etc.)</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<!-- include a script for adding stuff to the end of proofs. -->
<script>
// Get all the proofs in the document.
proofs = document.getElementsByClassName("proof");
// For each of the proofs, attach a floating child element in the bottom-right
// corner.
for (var proof of proofs) {
// Create a proof-ending tombstone.
square = document.createElement("div");
square.className = "tombstone";
square.innerHTML = "◼️";
// Attach the tombstone to the proof.
proof.appendChild(square);
}
</script>
<header>
<a class="homelink" rel="home" title="gerrytools" href="https://github.com/mggg/gerrytools">
<style>
header > h1 { display: none; }
img.resize {
max-width: 80%;
max-height: 80%;
display: block;
margin: 0 auto;
}
div.proof {
border: 1px solid black;
padding: 0em 1em;
width: 90%;
margin: 1em auto;
}
.tombstone {
margin-top: -2em;
float: right;
}
</style>
<img class="resize" src="https://mggg.org/assets/logo.svg" alt="MGGG Logo">
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gerrytools" href="../index.html">gerrytools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="gerrytools.data.URLs" href="URLs.html">gerrytools.data.URLs</a></code></li>
<li><code><a title="gerrytools.data.acs" href="acs.html">gerrytools.data.acs</a></code></li>
<li><code><a title="gerrytools.data.census" href="census.html">gerrytools.data.census</a></code></li>
<li><code><a title="gerrytools.data.estimatecvap" href="estimatecvap.html">gerrytools.data.estimatecvap</a></code></li>
<li><code><a title="gerrytools.data.fetch" href="fetch.html">gerrytools.data.fetch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="gerrytools.data.acs5" href="#gerrytools.data.acs5">acs5</a></code></li>
<li><code><a title="gerrytools.data.census10" href="#gerrytools.data.census10">census10</a></code></li>
<li><code><a title="gerrytools.data.census20" href="#gerrytools.data.census20">census20</a></code></li>
<li><code><a title="gerrytools.data.csvs" href="#gerrytools.data.csvs">csvs</a></code></li>
<li><code><a title="gerrytools.data.cvap" href="#gerrytools.data.cvap">cvap</a></code></li>
<li><code><a title="gerrytools.data.estimatecvap2010" href="#gerrytools.data.estimatecvap2010">estimatecvap2010</a></code></li>
<li><code><a title="gerrytools.data.estimatecvap2020" href="#gerrytools.data.estimatecvap2020">estimatecvap2020</a></code></li>
<li><code><a title="gerrytools.data.fetchgeometries" href="#gerrytools.data.fetchgeometries">fetchgeometries</a></code></li>
<li><code><a title="gerrytools.data.ids" href="#gerrytools.data.ids">ids</a></code></li>
<li><code><a title="gerrytools.data.one" href="#gerrytools.data.one">one</a></code></li>
<li><code><a title="gerrytools.data.remap" href="#gerrytools.data.remap">remap</a></code></li>
<li><code><a title="gerrytools.data.submissions" href="#gerrytools.data.submissions">submissions</a></code></li>
<li><code><a title="gerrytools.data.tabularized" href="#gerrytools.data.tabularized">tabularized</a></code></li>
<li><code><a title="gerrytools.data.variables" href="#gerrytools.data.variables">variables</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gerrytools.data.AssignmentCompressor" href="#gerrytools.data.AssignmentCompressor">AssignmentCompressor</a></code></h4>
<ul class="">
<li><code><a title="gerrytools.data.AssignmentCompressor.compress" href="#gerrytools.data.AssignmentCompressor.compress">compress</a></code></li>
<li><code><a title="gerrytools.data.AssignmentCompressor.compress_all" href="#gerrytools.data.AssignmentCompressor.compress_all">compress_all</a></code></li>
<li><code><a title="gerrytools.data.AssignmentCompressor.decompress" href="#gerrytools.data.AssignmentCompressor.decompress">decompress</a></code></li>
<li><code><a title="gerrytools.data.AssignmentCompressor.match" href="#gerrytools.data.AssignmentCompressor.match">match</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gerrytools.data.Submission" href="#gerrytools.data.Submission">Submission</a></code></h4>
<ul class="two-column">
<li><code><a title="gerrytools.data.Submission.id" href="#gerrytools.data.Submission.id">id</a></code></li>
<li><code><a title="gerrytools.data.Submission.link" href="#gerrytools.data.Submission.link">link</a></code></li>
<li><code><a title="gerrytools.data.Submission.plan" href="#gerrytools.data.Submission.plan">plan</a></code></li>
<li><code><a title="gerrytools.data.Submission.tileset" href="#gerrytools.data.Submission.tileset">tileset</a></code></li>
<li><code><a title="gerrytools.data.Submission.type" href="#gerrytools.data.Submission.type">type</a></code></li>
<li><code><a title="gerrytools.data.Submission.units" href="#gerrytools.data.Submission.units">units</a></code></li>
<li><code><a title="gerrytools.data.Submission.unitsType" href="#gerrytools.data.Submission.unitsType">unitsType</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>