<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>evaltools.auxiliary.AssignmentCompressor API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>evaltools.auxiliary.AssignmentCompressor</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from sortedcontainers import SortedDict, SortedList
import zlib

class AssignmentCompressor:
    &#34;&#34;&#34;
    A class for compressing and decompressing lots of assignments very, very
    quickly. Intended for use with ``jsonlines``-like libraries (where assignments
    are read in line-by-line) or for network requests (where assignments are 
    retrieved one-by-one). When decompressing, yields ``dict``s where keys are
    in sorted order.

    The compression schema considers the set of unique identifiers, imposes an
    ordering (lexicographic order) on the identifiers, and matches the assignment
    to that ordering. We assign all unassigned units to ``&#34;-1&#34;`` and, once the
    default cache size is hit (or assignments are no longer being read in),
    compress all assignments in the cache. Assignments are read in and out in
    the same order, and the keys for each assignment are in the same order.

    Example:
        To compress assignments, we need a set of unique identifiers such that
        each identifier maps one geometric unit to one district.

            ...

            geoids = blocks[&#34;GEOID20&#34;].astype(str)
            ac = AssignmentCompressor(geoids, location=&#34;compressed-assignments.ac&#34;)

            with ac as compressor:
                for assignment in assignments:
                    # Here, ensure that all assignments have string keys and
                    # string values; also ensure that an assignment&#39;s keys are
                    # a subset of geoids (or whatever IDs you&#39;re passing).
                    compressor.compress(assignment)

            ...

        To decompress assignments, we again must have a set of unique geometric
        identifiers which match the assignments. We can then iterate over the
        decompressed assignments as they&#39;re read out of the file.

            ...

            geoids = blocks[&#34;GEOID20&#34;].astype(str)
            ac = AssignmentCompressor(geoids, location=&#34;compressed-assignments.ac&#34;)

            for assignment in ac.decompress():
                &lt;do whatever!&gt;

            ...

    Attributes:
        DISTRICT_SEPARATOR: A bytestring which separates district identifiers in
            an assignment.
        ASSIGNMENT_SEPARATOR: A bytestring which separates assignments from
            each other.
        CHUNK_SEPARATOR: A bytestring which separates assignment chunks from each
            other.
        CHUNK_SIZE: Default number of bytes read in from the IO stream at each
            step.
        ENCODING: Default string encoding style.
        identifiers: A sortable, iterable collection of unique items corresponding
            to geographic identifiers.
        compressed: A pandas `Index` containing the identifiers; this is used
            to quickly perform vectorized identifier matchings, rather than using
            traditional iterative methods.
        cache: Collection of assignments to be compressed. Assignments are loaded
            into the cache every time the ``.compress()`` method is called, and
            is cleared whenever the length of the cache exceeds the window width.
        window: Maximum cache length before the cache is compressed, written to
            file, and emptied.
        default: The default assignment which is updated each time an assignment
            is passed to the compressor.
    &#34;&#34;&#34;

    def __init__(self, identifiers, window=10, location=&#34;compressed.ac&#34;):
        &#34;&#34;&#34;
        Creates `AssignmentCompressor` instance.

        Args:
            identifiers: An iterable collection of string identifiers; any
                assignment&#39;s keys must be a subset of `identifiers`.
            window: A positive integer representing the cache window size. Defaults
                to cache.
            location: The path to the compressed resource (read or write). Defaults
                to `compressed.ac`.
        &#34;&#34;&#34;
        self.identifiers = SortedList(identifiers)
        self.default = frozenset(zip(self.identifiers, [&#34;-1&#34;]*len(self.identifiers)))
        self.cache = []
        self.location = location

        # Error to users if the window is nonexistent.
        if type(window) != int or window &lt;= 0:
            raise ValueError(&#34;Cache window width must be a positive integer.&#34;)

        self.window = window

        self.DISTRICT_DELIMITER = b&#34;,&#34;
        self.ASSIGNMENT_DELIMITER = b&#34;&lt;&lt;&lt;*&gt;&gt;&gt;&#34;
        self.CHUNK_DELIMITER = b&#34;(((*)))&#34;
        self.CHUNK_SIZE = 16384
        self.ENCODING = &#34;raw_unicode_escape&#34;

    def match(self, assignment):
        &#34;&#34;&#34;
        Matches an assignment to an index (the set of geometric identifiers)
        and returns a `SortedDict`.

        Args:
            assignment: Dictionary which matches geometric identifiers to
                districts. All keys and values in this dictionary must be strings.

        &#34;&#34;&#34;
        # Create a dictionary which maps identifiers to `-1`, and update our
        # dictionary with assignment values.
        indexer = SortedDict(self.default)
        indexer.update(assignment)
        
        return indexer

    def __enter__(self):
        &#34;&#34;&#34;
        A simple context-management method. Allows the user to use `with`
        statements when compressing stuff so we don&#39;t have to worry about the
        user specifying the last item they&#39;ll be compressing.
        &#34;&#34;&#34;
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        &#34;&#34;&#34;
        Teardown. Once we&#39;ve exited the `with` statement (i.e. the user&#39;s all done
        feeding items to the compressor) we can force the remaining items to be
        compressed and written to file.
        &#34;&#34;&#34;
        if self.cache: self._compress(force=True)


    def compress(self, assignment):
        &#34;&#34;&#34;
        Compresses the assignment `assignment` using ``zlib``.

        Args:
            assignment: Dictionary which matches geometric identifiers to districts.
                All keys and values in this dictionary must be strings.
        &#34;&#34;&#34;
        # If the user provides an empty assignment or the assignment&#39;s keys aren&#39;t
        # a subset of `identifiers`, warn the user and skip the assignment.
        skip = False

        if not assignment:
            skip = True
            print(&#34;`assignment` is empty; skipping.&#34;)

        if not set(assignment.keys()).issubset(self.identifiers):
            skip = True
            print(
                &#34;`assignment`&#39;s keys are not a subset of `identifiers`; skipping. &#34; + \
                &#34;Please ensure that all keys and values in `assignment` are strings.&#34;,
            )

        # Join the things on the district separator, encode the whole thing, and
        # encode according to the default encoding.
        if not skip:
            indexed = self.match(assignment)
            sep = self.DISTRICT_DELIMITER.decode()
            encoded = bytes(sep.join(indexed.values()).encode(self.ENCODING))

            # Compress.
            self.cache += [encoded]
            self._compress()

    def _compress(self, force=False):
        &#34;&#34;&#34;
        Private method which actually does the compression.

        Args:
            force: If truthy, then the length of the cache is ignored, the data
                in the cache are compressed, and the compressed data is written
                to file. ``force`` is truthy when teardown logic is entered
                (i.e. after ``__exit()__`` is called).
        &#34;&#34;&#34;
        # Check to see if the cache is full. If so, compress the data, write to
        # file, and reset the cache.
        if len(self.cache) == self.window or force:
            with open(self.location, &#34;ab&#34;) as writer:
                # Write compressed data to file.
                compressed = zlib.compress(
                    bytes(self.ASSIGNMENT_DELIMITER.join(self.cache))
                )
                writer.write(compressed)

                # We only forcibly write the chunk separator to file if we&#39;ve
                # entered teardown logic and the cache *is not empty*. If the
                # cache is empty when we&#39;re entering teardown logic, that means
                # (number of assignments compressed) == (window width), in which
                # case we&#39;ve reached the end of compression and should not write
                # a separator; doing so will produce an empty bytestring (which,
                # in turn, produces a dictionary with one key, corresponding to
                # a null assignment).
                if not force: writer.write(self.CHUNK_DELIMITER)

            # Reset the cache.
            self.cache = []

    def decompress(self):
        &#34;&#34;&#34;
        Decompresses the data at ``location``. A generator which ``yield``s
        assignments.

        Yields:
            Yields decompressed assignment dictionaries.
        &#34;&#34;&#34;
        # Open the compressed file. Then we read it in chunks, loading until
        # we hit our separator or until the end of the file.
        with open(self.location, &#34;rb&#34;) as _compressed_fin:
            for chunk in self._chunk(_compressed_fin):
                if not chunk: break
                for assignment in self._decompress(chunk): yield assignment

    def _chunk(self, stream):
        &#34;&#34;&#34;
        Private method for reading chunks from file without holding the entire
        file in memory. A generator for decompressed assignments.

        Args:
            stream: A ``BytesIO`` instance from which data is read.
        &#34;&#34;&#34;
        # Create a buffer.
        _buffer = []

        # Read until we hit the end of the file `yield`ing each chunk as we go.
        while True:
            # Read in a chunk of data.
            chunk = stream.read(self.CHUNK_SIZE)
            _buffer.append(chunk)

            # Check if the chunk has our delimiter in it. If it contains our
            # delimiter, then the buffer *up to the delimiter* contains compressed
            # assignments; this should be `yield`ed and decompressed. We only
            # want to get the part before the delimiter for decompression, but
            # retain the rest.
            if self.CHUNK_DELIMITER in chunk:
                _buffered_bytes = b&#34;&#34;.join(_buffer)
                part, _buffer_first = _buffered_bytes.split(self.CHUNK_DELIMITER, 1)
                _buffer = [_buffer_first]
                yield part

            # If the chunk&#39;s empty, `yield` the remaining buffer and return.
            if not chunk: yield b&#34;&#34;.join(_buffer); break

    def _decompress(self, chunk):
        &#34;&#34;&#34;
        Private method which decompresses assignments.

        Args:
            chunk: Compressed, byte-encoded data representing ``window``
                assignments.

        Returns:
            List of decompressed assignment objects.
        &#34;&#34;&#34;
        # Decompress the chunk and split it on our delimiter.
        decompressed = zlib.decompress(chunk)
        decompressed_parts = decompressed.split(self.ASSIGNMENT_DELIMITER)
        
        # For each of the parts, decode the bytes, make them into lists, and
        # match them to GEOIDs.
        decoded_parts = [part.decode() for part in decompressed_parts]
        split_parts = [part.split(self.DISTRICT_DELIMITER.decode()) for part in decoded_parts]
        indexed_parts = [dict(zip(self.identifiers, part)) for part in split_parts]

        return indexed_parts</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor"><code class="flex name class">
<span>class <span class="ident">AssignmentCompressor</span></span>
<span>(</span><span>identifiers, window=10, location='compressed.ac')</span>
</code></dt>
<dd>
<div class="desc"><p>A class for compressing and decompressing lots of assignments very, very
quickly. Intended for use with <code>jsonlines</code>-like libraries (where assignments
are read in line-by-line) or for network requests (where assignments are
retrieved one-by-one). When decompressing, yields <code>dict</code>s where keys are
in sorted order.</p>
<p>The compression schema considers the set of unique identifiers, imposes an
ordering (lexicographic order) on the identifiers, and matches the assignment
to that ordering. We assign all unassigned units to <code>"-1"</code> and, once the
default cache size is hit (or assignments are no longer being read in),
compress all assignments in the cache. Assignments are read in and out in
the same order, and the keys for each assignment are in the same order.</p>
<h2 id="example">Example</h2>
<p>To compress assignments, we need a set of unique identifiers such that
each identifier maps one geometric unit to one district.</p>
<pre><code>...

geoids = blocks["GEOID20"].astype(str)
ac = AssignmentCompressor(geoids, location="compressed-assignments.ac")

with ac as compressor:
    for assignment in assignments:
        # Here, ensure that all assignments have string keys and
        # string values; also ensure that an assignment's keys are
        # a subset of geoids (or whatever IDs you're passing).
        compressor.compress(assignment)

...
</code></pre>
<p>To decompress assignments, we again must have a set of unique geometric
identifiers which match the assignments. We can then iterate over the
decompressed assignments as they're read out of the file.</p>
<pre><code>...

geoids = blocks["GEOID20"].astype(str)
ac = AssignmentCompressor(geoids, location="compressed-assignments.ac")

for assignment in ac.decompress():
    &lt;do whatever!&gt;

...
</code></pre>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>DISTRICT_SEPARATOR</code></strong></dt>
<dd>A bytestring which separates district identifiers in
an assignment.</dd>
<dt><strong><code>ASSIGNMENT_SEPARATOR</code></strong></dt>
<dd>A bytestring which separates assignments from
each other.</dd>
<dt><strong><code>CHUNK_SEPARATOR</code></strong></dt>
<dd>A bytestring which separates assignment chunks from each
other.</dd>
<dt><strong><code>CHUNK_SIZE</code></strong></dt>
<dd>Default number of bytes read in from the IO stream at each
step.</dd>
<dt><strong><code>ENCODING</code></strong></dt>
<dd>Default string encoding style.</dd>
<dt><strong><code>identifiers</code></strong></dt>
<dd>A sortable, iterable collection of unique items corresponding
to geographic identifiers.</dd>
<dt><strong><code>compressed</code></strong></dt>
<dd>A pandas <code>Index</code> containing the identifiers; this is used
to quickly perform vectorized identifier matchings, rather than using
traditional iterative methods.</dd>
<dt><strong><code>cache</code></strong></dt>
<dd>Collection of assignments to be compressed. Assignments are loaded
into the cache every time the <code>.compress()</code> method is called, and
is cleared whenever the length of the cache exceeds the window width.</dd>
<dt><strong><code>window</code></strong></dt>
<dd>Maximum cache length before the cache is compressed, written to
file, and emptied.</dd>
<dt><strong><code>default</code></strong></dt>
<dd>The default assignment which is updated each time an assignment
is passed to the compressor.</dd>
</dl>
<p>Creates <code><a title="evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor" href="#evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor">AssignmentCompressor</a></code> instance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>identifiers</code></strong></dt>
<dd>An iterable collection of string identifiers; any
assignment's keys must be a subset of <code>identifiers</code>.</dd>
<dt><strong><code>window</code></strong></dt>
<dd>A positive integer representing the cache window size. Defaults
to cache.</dd>
<dt><strong><code>location</code></strong></dt>
<dd>The path to the compressed resource (read or write). Defaults
to <code>compressed.ac</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AssignmentCompressor:
    &#34;&#34;&#34;
    A class for compressing and decompressing lots of assignments very, very
    quickly. Intended for use with ``jsonlines``-like libraries (where assignments
    are read in line-by-line) or for network requests (where assignments are 
    retrieved one-by-one). When decompressing, yields ``dict``s where keys are
    in sorted order.

    The compression schema considers the set of unique identifiers, imposes an
    ordering (lexicographic order) on the identifiers, and matches the assignment
    to that ordering. We assign all unassigned units to ``&#34;-1&#34;`` and, once the
    default cache size is hit (or assignments are no longer being read in),
    compress all assignments in the cache. Assignments are read in and out in
    the same order, and the keys for each assignment are in the same order.

    Example:
        To compress assignments, we need a set of unique identifiers such that
        each identifier maps one geometric unit to one district.

            ...

            geoids = blocks[&#34;GEOID20&#34;].astype(str)
            ac = AssignmentCompressor(geoids, location=&#34;compressed-assignments.ac&#34;)

            with ac as compressor:
                for assignment in assignments:
                    # Here, ensure that all assignments have string keys and
                    # string values; also ensure that an assignment&#39;s keys are
                    # a subset of geoids (or whatever IDs you&#39;re passing).
                    compressor.compress(assignment)

            ...

        To decompress assignments, we again must have a set of unique geometric
        identifiers which match the assignments. We can then iterate over the
        decompressed assignments as they&#39;re read out of the file.

            ...

            geoids = blocks[&#34;GEOID20&#34;].astype(str)
            ac = AssignmentCompressor(geoids, location=&#34;compressed-assignments.ac&#34;)

            for assignment in ac.decompress():
                &lt;do whatever!&gt;

            ...

    Attributes:
        DISTRICT_SEPARATOR: A bytestring which separates district identifiers in
            an assignment.
        ASSIGNMENT_SEPARATOR: A bytestring which separates assignments from
            each other.
        CHUNK_SEPARATOR: A bytestring which separates assignment chunks from each
            other.
        CHUNK_SIZE: Default number of bytes read in from the IO stream at each
            step.
        ENCODING: Default string encoding style.
        identifiers: A sortable, iterable collection of unique items corresponding
            to geographic identifiers.
        compressed: A pandas `Index` containing the identifiers; this is used
            to quickly perform vectorized identifier matchings, rather than using
            traditional iterative methods.
        cache: Collection of assignments to be compressed. Assignments are loaded
            into the cache every time the ``.compress()`` method is called, and
            is cleared whenever the length of the cache exceeds the window width.
        window: Maximum cache length before the cache is compressed, written to
            file, and emptied.
        default: The default assignment which is updated each time an assignment
            is passed to the compressor.
    &#34;&#34;&#34;

    def __init__(self, identifiers, window=10, location=&#34;compressed.ac&#34;):
        &#34;&#34;&#34;
        Creates `AssignmentCompressor` instance.

        Args:
            identifiers: An iterable collection of string identifiers; any
                assignment&#39;s keys must be a subset of `identifiers`.
            window: A positive integer representing the cache window size. Defaults
                to cache.
            location: The path to the compressed resource (read or write). Defaults
                to `compressed.ac`.
        &#34;&#34;&#34;
        self.identifiers = SortedList(identifiers)
        self.default = frozenset(zip(self.identifiers, [&#34;-1&#34;]*len(self.identifiers)))
        self.cache = []
        self.location = location

        # Error to users if the window is nonexistent.
        if type(window) != int or window &lt;= 0:
            raise ValueError(&#34;Cache window width must be a positive integer.&#34;)

        self.window = window

        self.DISTRICT_DELIMITER = b&#34;,&#34;
        self.ASSIGNMENT_DELIMITER = b&#34;&lt;&lt;&lt;*&gt;&gt;&gt;&#34;
        self.CHUNK_DELIMITER = b&#34;(((*)))&#34;
        self.CHUNK_SIZE = 16384
        self.ENCODING = &#34;raw_unicode_escape&#34;

    def match(self, assignment):
        &#34;&#34;&#34;
        Matches an assignment to an index (the set of geometric identifiers)
        and returns a `SortedDict`.

        Args:
            assignment: Dictionary which matches geometric identifiers to
                districts. All keys and values in this dictionary must be strings.

        &#34;&#34;&#34;
        # Create a dictionary which maps identifiers to `-1`, and update our
        # dictionary with assignment values.
        indexer = SortedDict(self.default)
        indexer.update(assignment)
        
        return indexer

    def __enter__(self):
        &#34;&#34;&#34;
        A simple context-management method. Allows the user to use `with`
        statements when compressing stuff so we don&#39;t have to worry about the
        user specifying the last item they&#39;ll be compressing.
        &#34;&#34;&#34;
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        &#34;&#34;&#34;
        Teardown. Once we&#39;ve exited the `with` statement (i.e. the user&#39;s all done
        feeding items to the compressor) we can force the remaining items to be
        compressed and written to file.
        &#34;&#34;&#34;
        if self.cache: self._compress(force=True)


    def compress(self, assignment):
        &#34;&#34;&#34;
        Compresses the assignment `assignment` using ``zlib``.

        Args:
            assignment: Dictionary which matches geometric identifiers to districts.
                All keys and values in this dictionary must be strings.
        &#34;&#34;&#34;
        # If the user provides an empty assignment or the assignment&#39;s keys aren&#39;t
        # a subset of `identifiers`, warn the user and skip the assignment.
        skip = False

        if not assignment:
            skip = True
            print(&#34;`assignment` is empty; skipping.&#34;)

        if not set(assignment.keys()).issubset(self.identifiers):
            skip = True
            print(
                &#34;`assignment`&#39;s keys are not a subset of `identifiers`; skipping. &#34; + \
                &#34;Please ensure that all keys and values in `assignment` are strings.&#34;,
            )

        # Join the things on the district separator, encode the whole thing, and
        # encode according to the default encoding.
        if not skip:
            indexed = self.match(assignment)
            sep = self.DISTRICT_DELIMITER.decode()
            encoded = bytes(sep.join(indexed.values()).encode(self.ENCODING))

            # Compress.
            self.cache += [encoded]
            self._compress()

    def _compress(self, force=False):
        &#34;&#34;&#34;
        Private method which actually does the compression.

        Args:
            force: If truthy, then the length of the cache is ignored, the data
                in the cache are compressed, and the compressed data is written
                to file. ``force`` is truthy when teardown logic is entered
                (i.e. after ``__exit()__`` is called).
        &#34;&#34;&#34;
        # Check to see if the cache is full. If so, compress the data, write to
        # file, and reset the cache.
        if len(self.cache) == self.window or force:
            with open(self.location, &#34;ab&#34;) as writer:
                # Write compressed data to file.
                compressed = zlib.compress(
                    bytes(self.ASSIGNMENT_DELIMITER.join(self.cache))
                )
                writer.write(compressed)

                # We only forcibly write the chunk separator to file if we&#39;ve
                # entered teardown logic and the cache *is not empty*. If the
                # cache is empty when we&#39;re entering teardown logic, that means
                # (number of assignments compressed) == (window width), in which
                # case we&#39;ve reached the end of compression and should not write
                # a separator; doing so will produce an empty bytestring (which,
                # in turn, produces a dictionary with one key, corresponding to
                # a null assignment).
                if not force: writer.write(self.CHUNK_DELIMITER)

            # Reset the cache.
            self.cache = []

    def decompress(self):
        &#34;&#34;&#34;
        Decompresses the data at ``location``. A generator which ``yield``s
        assignments.

        Yields:
            Yields decompressed assignment dictionaries.
        &#34;&#34;&#34;
        # Open the compressed file. Then we read it in chunks, loading until
        # we hit our separator or until the end of the file.
        with open(self.location, &#34;rb&#34;) as _compressed_fin:
            for chunk in self._chunk(_compressed_fin):
                if not chunk: break
                for assignment in self._decompress(chunk): yield assignment

    def _chunk(self, stream):
        &#34;&#34;&#34;
        Private method for reading chunks from file without holding the entire
        file in memory. A generator for decompressed assignments.

        Args:
            stream: A ``BytesIO`` instance from which data is read.
        &#34;&#34;&#34;
        # Create a buffer.
        _buffer = []

        # Read until we hit the end of the file `yield`ing each chunk as we go.
        while True:
            # Read in a chunk of data.
            chunk = stream.read(self.CHUNK_SIZE)
            _buffer.append(chunk)

            # Check if the chunk has our delimiter in it. If it contains our
            # delimiter, then the buffer *up to the delimiter* contains compressed
            # assignments; this should be `yield`ed and decompressed. We only
            # want to get the part before the delimiter for decompression, but
            # retain the rest.
            if self.CHUNK_DELIMITER in chunk:
                _buffered_bytes = b&#34;&#34;.join(_buffer)
                part, _buffer_first = _buffered_bytes.split(self.CHUNK_DELIMITER, 1)
                _buffer = [_buffer_first]
                yield part

            # If the chunk&#39;s empty, `yield` the remaining buffer and return.
            if not chunk: yield b&#34;&#34;.join(_buffer); break

    def _decompress(self, chunk):
        &#34;&#34;&#34;
        Private method which decompresses assignments.

        Args:
            chunk: Compressed, byte-encoded data representing ``window``
                assignments.

        Returns:
            List of decompressed assignment objects.
        &#34;&#34;&#34;
        # Decompress the chunk and split it on our delimiter.
        decompressed = zlib.decompress(chunk)
        decompressed_parts = decompressed.split(self.ASSIGNMENT_DELIMITER)
        
        # For each of the parts, decode the bytes, make them into lists, and
        # match them to GEOIDs.
        decoded_parts = [part.decode() for part in decompressed_parts]
        split_parts = [part.split(self.DISTRICT_DELIMITER.decode()) for part in decoded_parts]
        indexed_parts = [dict(zip(self.identifiers, part)) for part in split_parts]

        return indexed_parts</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor.compress"><code class="name flex">
<span>def <span class="ident">compress</span></span>(<span>self, assignment)</span>
</code></dt>
<dd>
<div class="desc"><p>Compresses the assignment <code>assignment</code> using <code>zlib</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>assignment</code></strong></dt>
<dd>Dictionary which matches geometric identifiers to districts.
All keys and values in this dictionary must be strings.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compress(self, assignment):
    &#34;&#34;&#34;
    Compresses the assignment `assignment` using ``zlib``.

    Args:
        assignment: Dictionary which matches geometric identifiers to districts.
            All keys and values in this dictionary must be strings.
    &#34;&#34;&#34;
    # If the user provides an empty assignment or the assignment&#39;s keys aren&#39;t
    # a subset of `identifiers`, warn the user and skip the assignment.
    skip = False

    if not assignment:
        skip = True
        print(&#34;`assignment` is empty; skipping.&#34;)

    if not set(assignment.keys()).issubset(self.identifiers):
        skip = True
        print(
            &#34;`assignment`&#39;s keys are not a subset of `identifiers`; skipping. &#34; + \
            &#34;Please ensure that all keys and values in `assignment` are strings.&#34;,
        )

    # Join the things on the district separator, encode the whole thing, and
    # encode according to the default encoding.
    if not skip:
        indexed = self.match(assignment)
        sep = self.DISTRICT_DELIMITER.decode()
        encoded = bytes(sep.join(indexed.values()).encode(self.ENCODING))

        # Compress.
        self.cache += [encoded]
        self._compress()</code></pre>
</details>
</dd>
<dt id="evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor.decompress"><code class="name flex">
<span>def <span class="ident">decompress</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Decompresses the data at <code>location</code>. A generator which <code>yield</code>s
assignments.</p>
<h2 id="yields">Yields</h2>
<p>Yields decompressed assignment dictionaries.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decompress(self):
    &#34;&#34;&#34;
    Decompresses the data at ``location``. A generator which ``yield``s
    assignments.

    Yields:
        Yields decompressed assignment dictionaries.
    &#34;&#34;&#34;
    # Open the compressed file. Then we read it in chunks, loading until
    # we hit our separator or until the end of the file.
    with open(self.location, &#34;rb&#34;) as _compressed_fin:
        for chunk in self._chunk(_compressed_fin):
            if not chunk: break
            for assignment in self._decompress(chunk): yield assignment</code></pre>
</details>
</dd>
<dt id="evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor.match"><code class="name flex">
<span>def <span class="ident">match</span></span>(<span>self, assignment)</span>
</code></dt>
<dd>
<div class="desc"><p>Matches an assignment to an index (the set of geometric identifiers)
and returns a <code>SortedDict</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>assignment</code></strong></dt>
<dd>Dictionary which matches geometric identifiers to
districts. All keys and values in this dictionary must be strings.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def match(self, assignment):
    &#34;&#34;&#34;
    Matches an assignment to an index (the set of geometric identifiers)
    and returns a `SortedDict`.

    Args:
        assignment: Dictionary which matches geometric identifiers to
            districts. All keys and values in this dictionary must be strings.

    &#34;&#34;&#34;
    # Create a dictionary which maps identifiers to `-1`, and update our
    # dictionary with assignment values.
    indexer = SortedDict(self.default)
    indexer.update(assignment)
    
    return indexer</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="evaltools.auxiliary" href="index.html">evaltools.auxiliary</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor" href="#evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor">AssignmentCompressor</a></code></h4>
<ul class="">
<li><code><a title="evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor.compress" href="#evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor.compress">compress</a></code></li>
<li><code><a title="evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor.decompress" href="#evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor.decompress">decompress</a></code></li>
<li><code><a title="evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor.match" href="#evaltools.auxiliary.AssignmentCompressor.AssignmentCompressor.match">match</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>